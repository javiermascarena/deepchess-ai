{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aux_functions\n",
    "import importlib\n",
    "\n",
    "importlib.reload(aux_functions)\n",
    "from aux_functions import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, SubsetRandomSampler\n",
    "# For masking\n",
    "from torch.masked import masked_tensor\n",
    "\n",
    "import numpy as np\n",
    "import chess\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PROCESSING**\n",
    "\n",
    "- Importing the pgn data\n",
    "- Transforming the data to sparce tensors \n",
    "- Splitting the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "[509, 580, 420, 75, 519, 102, 373, 494, 278, 366, 383, 97, 348, 56, 83, 359, 47, 341, 344, 260, 242, 590, 146, 141, 455, 340, 60, 524, 558, 311, 217, 563, 99, 237, 235, 100, 583, 176, 214, 487, 577, 84, 409, 208, 178, 13, 554, 514, 185, 552, 295, 440, 181, 400, 407, 575, 584, 553, 263, 110, 64, 397, 142, 561, 408, 42, 65, 115, 121, 559, 226, 276, 4, 14, 73, 560, 470, 331, 11, 68, 370, 51, 253, 316, 177, 254, 445, 53, 17, 513, 589, 174, 108, 169, 288, 399, 427, 8, 259, 36, 193, 127, 157, 106, 488, 255, 234, 398, 551, 35, 82, 475, 390, 285, 145, 385, 585, 339, 175, 67, 159, 484, 508, 292, 451, 207, 250, 244, 120, 130, 422, 342, 229, 231, 143, 256, 301, 90, 456, 15, 432, 568, 305, 569, 95, 549, 34, 536, 482, 424, 567, 109, 522, 466, 134, 16, 467, 544, 265, 81, 550, 537, 395, 469, 122, 468, 372, 86, 43, 191, 379, 57, 160, 303, 306, 201, 542, 299, 528, 417, 116, 349, 382, 393, 94, 22, 539, 118, 461, 435, 168, 0, 71, 196, 165, 571, 418, 586, 345, 351, 576, 389, 156, 273, 471, 352, 79, 55, 158, 287, 413, 152, 480, 369, 114, 199, 27, 45, 460, 19, 12, 269, 147, 498, 223, 538, 138, 52, 124, 172, 225, 258, 555, 493, 459, 74, 499, 50, 222, 10, 478, 91, 518, 23, 216, 448, 329, 327, 24, 411, 294, 406, 163, 429, 70, 430, 428, 314, 3, 267, 167, 486, 243, 375, 205, 206, 190, 171, 129, 20, 246, 87, 450, 523, 2, 139, 233, 363, 30, 457, 496, 7, 332, 54, 337, 18, 545, 137, 170, 5, 277, 28, 33, 462, 525, 232, 69, 453, 117, 388, 173, 307, 161, 343, 236, 380, 317, 520, 136, 135, 463, 557, 204, 48, 347, 578, 210, 318, 415, 262, 358, 556, 444, 521, 534, 495, 279, 490, 570, 105, 360, 582, 402, 454, 572, 396, 247, 274, 354, 357, 32, 302, 483, 452, 565, 293, 195, 39, 449, 507, 479, 541, 365, 489, 113, 326, 144, 410, 281, 464, 419, 218, 527, 546, 266, 515, 200, 125, 252, 315, 248, 587, 1, 503, 183, 588, 566, 272, 502, 492, 275, 188, 353, 529, 6, 194, 500, 330, 392, 322, 25, 268, 284, 414, 504, 426, 290, 154, 386, 289, 37, 245, 423, 227, 321, 140, 179, 62, 66, 338, 198, 394, 412, 313, 501, 328, 72, 481, 126, 212, 148, 96, 89, 381, 362, 182, 443, 203, 93, 535, 491, 21, 505, 76, 300, 257, 107, 230, 46, 517, 312, 61, 579, 371]\n"
     ]
    }
   ],
   "source": [
    "TEST_PERCENT = 0.25\n",
    "\n",
    "# Load pgn paths\n",
    "pgns = import_data(1)\n",
    "\n",
    "# Convert pgns to tensors\n",
    "board_tensors, next_moves = parse_pgn_to_tensors(pgns)\n",
    "\n",
    "# Converting the dataset into a custom pytorch one\n",
    "dataset = ChessDataset(board_tensors, next_moves)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Splitting the data into train and test\n",
    "train_dataset, test_data = torch.utils.data.random_split(dataset, [1-TEST_PERCENT, TEST_PERCENT])\n",
    "\n",
    "print(len(test_data))  # Number of states\n",
    "print(train_dataset.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEURAL NETWORK DESIGN**\n",
    "- 2 Convolutional layers\n",
    "- 2 Fully connected hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to do the operations on the cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PieceToMoveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Takes as input a tensor of 14 channels (8x8 board)\n",
    "        self.conv1 = nn.Conv2d(14, 6, 3)  # 6 filters, 3x3 kernel\n",
    "        self.pool = nn.MaxPool2d(2, 2)    # Max pooling with 2x2 window\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)  # 16 filters, 3x3 kernel\n",
    "\n",
    "        # Using droput to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        # Using batch normalization to make training faster and more stable\n",
    "        self.bn1 = nn.BatchNorm1d(120)  # For the 1st layer\n",
    "        self.bn2 = nn.BatchNorm1d(84)   # For the 2nd layer\n",
    "        \n",
    "        # Output from conv2 will be (16 channels, 1x1 feature maps)\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # Predicts the tile to move the piece from (64 possible tiles on the board)\n",
    "        self.fc3 = nn.Linear(84, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Apply first conv + pooling\n",
    "        x = F.relu(self.conv2(x))             # Apply second conv to get (16 x 1 x 1)\n",
    "        x = torch.flatten(x, 1)               # Flatten all dimensions except batch size\n",
    "        x = F.relu(self.bn1(self.fc1(x)))     # Fully connected layer 1 and batch normalization\n",
    "        x = self.dropout(x)                   # Dropout of some first layer neurons\n",
    "        x = F.relu(self.bn2(self.fc2(x)))     # Fully connected layer 2\n",
    "        x = self.fc3(x)                       # Output layer (no activation, logits for classification)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initializing the network\n",
    "piece_to_move_net = PieceToMoveNet()\n",
    "# Move the network to gpu/cpu befor initializing the optimizer\n",
    "piece_to_move_net.to(device)\n",
    "\n",
    "# Adam optimizer will be used due to its versatility\n",
    "optimizer = optim.Adam(piece_to_move_net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING LOOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(tensor)-> list:\n",
    "    \"\"\"Generates a mask which contains the position of the pieces that can move\"\"\"\n",
    "\n",
    "    # Contain a 2D representation of the board\n",
    "    # board_mask = torch.zeros((8,8))\n",
    "\n",
    "    # If layer 12 has any 1 it will be whites turn\n",
    "    if  torch.any(tensor[12] == 1):\n",
    "        # White pieces are in layers 0 to 5, apply a mask which will be 1 when there is a one\n",
    "        mask = torch.sum(tensor[0:6], dim = 0) # for summing across the layers\n",
    "\n",
    "\n",
    "    # If layer 13 has any 1 it will be blacks turn\n",
    "    elif torch.any(tensor[13] == 1):\n",
    "        # Black pieces are in range 6 to 11\n",
    "        # Apply a mask, if there is a piece it will be a 1\n",
    "        mask = torch.sum(tensor[6:12], dim = 0)\n",
    "\n",
    "\n",
    "    # A position wion zero value means a piece is present\n",
    "    # board_mask[mask > 0] = 1\n",
    "\n",
    "    # Flatten the board to a 1D array\n",
    "    # board_mask = board_mask.flatten().tolist()\n",
    "\n",
    "    return mask.flatten().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'mask' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 182\u001b[0m\n\u001b[0;32m    179\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/piece_to_move_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m    180\u001b[0m splits \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39mK, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 182\u001b[0m \u001b[43mtrain_multiple_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpiece_to_move_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 147\u001b[0m, in \u001b[0;36mtrain_multiple_folds\u001b[1;34m(n_epochs, batch_size, model, splits, writer, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m    144\u001b[0m val_sampler_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_sampler)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs): \n\u001b[1;32m--> 147\u001b[0m     train_loss, train_correct \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpiece_to_move_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sampler_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m     val_loss, val_correct \u001b[38;5;241m=\u001b[39m validation_epoch(piece_to_move_net, val_loader, loss_fn, val_sampler_size)\n\u001b[0;32m    150\u001b[0m     avg_tloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n",
      "Cell \u001b[1;32mIn[46], line 23\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, train_loader, loss_fn, train_sampler_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calculating model's output\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mgenerate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     24\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mask)\n\u001b[0;32m     25\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[46], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calculating model's output\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m mask \u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m     24\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mask)\n\u001b[0;32m     25\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[45], line 26\u001b[0m, in \u001b[0;36mgenerate_mask\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m     17\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(tensor[\u001b[38;5;241m6\u001b[39m:\u001b[38;5;241m12\u001b[39m], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# A position wion zero value means a piece is present\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# board_mask[mask > 0] = 1\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Flatten the board to a 1D array\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# board_mask = board_mask.flatten().tolist()\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmask\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'mask' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, loss_fn, train_sampler_size): \n",
    "    \"\"\"\n",
    "    Trains the model for one epoch and returns the average training loss and accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    running_loss = 0.  \n",
    "    running_correct = 0.\n",
    "\n",
    "    # Looping through all samples in a batch\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Extracting the board tensor\n",
    "        inputs = data[0]\n",
    "        # Extracting the tile of the piece to move\n",
    "        labels = data[1]\n",
    "\n",
    "        # Resetting the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculating model's output\n",
    "        mask = [generate_mask(pos) for pos in inputs]\n",
    "        mask = torch.tensor(mask)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        \"\"\"print(inputs[0])\n",
    "        print(labels[0])\n",
    "        print(mask[0])\"\"\"\n",
    "\n",
    "        # Batch size \n",
    "        for j in range(len(mask)):\n",
    "            for h in range(len(labels)):\n",
    "                piece = mask[j, labels[h]]\n",
    "                if piece == 0:\n",
    "                    print(inputs[j])\n",
    "                    print(labels[h])\n",
    "                    print(mask[j])\n",
    "\n",
    "\n",
    "        # Moving inputs and labels to the gpu/cpu\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        outputs = logits * mask.float()\n",
    "\n",
    "        # Calculating the sample loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Calculating the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Updating model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adding the last loss to the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate number of correct predictions\n",
    "        _, predictions = torch.max(outputs.data, 1)\n",
    "        running_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    # Averaging the loss for all samples in the batch\n",
    "    running_loss /= (i + 1)\n",
    "\n",
    "    # Calculate accuracy based on the total samples in the fold (train_sampler_size)\n",
    "    train_accuracy = running_correct / train_sampler_size\n",
    "\n",
    "    return running_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validation_epoch(model, validation_loader, loss_fn, val_sampler_size):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch and returns the average validation loss and accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    running_vloss = 0.\n",
    "    running_vcorrect = 0.\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations for validation set\n",
    "    with torch.no_grad():\n",
    "        # Looping through all batches in the validation set\n",
    "        for i, v_data in enumerate(validation_loader):\n",
    "            # Getting the tensors of the validation data\n",
    "            vinputs = v_data[0]\n",
    "            vlabels = v_data[1] \n",
    "\n",
    "            # Calculating the output of the model\n",
    "            mask = [generate_mask(pos) for pos in vinputs]\n",
    "            mask = torch.tensor(mask)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            # Moving inputs and labels to the gpu/cpu\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "\n",
    "            logits = model(vinputs)\n",
    "            voutputs = logits * mask.float()\n",
    "\n",
    "            # Calculating the loss of the model in the validation sample\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            # Adding this sample's loss to the total loss\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "            # Calculate number of correct predictions\n",
    "            _, predictions = torch.max(voutputs.data, 1)\n",
    "            running_vcorrect += (predictions == vlabels).sum().item()\n",
    "\n",
    "    # Averaging the loss for all samples in the validation set\n",
    "    running_vloss /= (i + 1)\n",
    "\n",
    "    # Calculate accuracy based on the total samples in the fold (val_sampler_size)\n",
    "    validation_accuracy = running_vcorrect / val_sampler_size\n",
    "\n",
    "    return running_vloss, validation_accuracy\n",
    "\n",
    "\n",
    "def train_multiple_folds(n_epochs, batch_size, model, splits, writer, optimizer, loss_fn):\n",
    "\n",
    "    best_fold_vloss = 1_000\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "        print(f\"FOLD {fold+1}\")\n",
    "\n",
    "        avg_tloss = 0.\n",
    "        avg_tacc = 0.\n",
    "        avg_vloss = 0.\n",
    "        avg_vacc = 0.\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        train_sampler_size = len(train_sampler)\n",
    "        val_sampler_size = len(val_sampler)\n",
    "\n",
    "        for epoch in range(n_epochs): \n",
    "            train_loss, train_correct = train_epoch(piece_to_move_net, optimizer, train_loader, loss_fn, train_sampler_size)\n",
    "            val_loss, val_correct = validation_epoch(piece_to_move_net, val_loader, loss_fn, val_sampler_size)\n",
    "\n",
    "            avg_tloss += train_loss\n",
    "            avg_tacc += train_correct\n",
    "            avg_vloss += val_loss\n",
    "            avg_vacc += val_correct\n",
    "\n",
    "            print(f\"Epoch: {epoch} Train Loss: {train_loss}, Valid Loss: {val_loss} | Train Acc: {train_correct}, Valid Acc: {val_correct}\")\n",
    "\n",
    "\n",
    "        avg_tloss /= (epoch + 1)\n",
    "        avg_tacc /= (epoch + 1)\n",
    "        avg_vloss /= (epoch + 1)\n",
    "        avg_vacc /= (epoch + 1)\n",
    "\n",
    "        # Adding insights\n",
    "        writer.add_scalars(\"Loss\", {\"Training\": avg_tloss, \"Validation\": avg_vloss}, fold + 1)\n",
    "        writer.add_scalars(\"Accuracy\", {\"Training\": avg_tacc, \"Validation\": avg_vacc}, fold + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Saving the model if the loss on the validation is lower than the best one\n",
    "        if avg_vloss < best_fold_vloss:\n",
    "            best_fold_vloss = avg_vloss\n",
    "            model_path = f\"models/piece_to_move_net_{timestamp}_{fold}\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "K = 5\n",
    "# Logs training statistics for TensorBoard visualization\n",
    "writer = SummaryWriter(f\"runs/piece_to_move_{timestamp}\")  \n",
    "splits = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "train_multiple_folds(EPOCHS, BATCH_SIZE, piece_to_move_net, splits, writer, optimizer, loss_fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
