{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aux_functions\n",
    "import importlib\n",
    "\n",
    "importlib.reload(aux_functions)\n",
    "from aux_functions import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, SubsetRandomSampler\n",
    "# For masking\n",
    "from torch.masked import masked_tensor\n",
    "\n",
    "import numpy as np\n",
    "import chess\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PROCESSING**\n",
    "\n",
    "- Importing the pgn data\n",
    "- Transforming the data to sparce tensors \n",
    "- Splitting the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845\n",
      "[414, 235, 1355, 1009, 437, 1064, 2105, 260, 2175, 81, 291, 79, 728, 2653, 2508, 1510, 2518, 2965, 2428, 1621, 2124, 763, 1293, 797, 1454, 1299, 443, 686, 967, 2616, 231, 283, 2045, 49, 2573, 717, 2412, 2931, 1205, 376, 678, 3326, 2258, 986, 1175, 861, 929, 1622, 3254, 3260, 3038, 408, 705, 1465, 949, 1752, 3331, 3113, 222, 365, 879, 128, 3065, 1309, 1241, 3233, 284, 2849, 2422, 2786, 1131, 1511, 92, 1568, 114, 2332, 2917, 1498, 1287, 1329, 126, 2037, 1326, 1383, 995, 1641, 3007, 3150, 293, 671, 2797, 206, 974, 3096, 2249, 229, 2462, 4, 2707, 1108, 35, 1747, 3098, 3055, 442, 1006, 2830, 2053, 393, 2667, 2633, 883, 1056, 1983, 1274, 1399, 3317, 100, 2969, 1744, 3337, 62, 1575, 2861, 1433, 715, 719, 2141, 2795, 2809, 446, 2584, 2959, 2831, 470, 1093, 2034, 381, 2264, 1448, 683, 1388, 1432, 1123, 1047, 941, 2257, 976, 959, 699, 790, 767, 2064, 2994, 2568, 2367, 833, 3168, 1400, 1325, 2240, 277, 2949, 1449, 2513, 3061, 1336, 1401, 3335, 290, 3339, 1367, 2817, 3114, 1415, 787, 2087, 2304, 464, 1535, 3207, 1555, 321, 2790, 2503, 1856, 1438, 2145, 3262, 2638, 1833, 157, 378, 2761, 1734, 1345, 1530, 2149, 2868, 113, 997, 1424, 2737, 2368, 2280, 1995, 1803, 1871, 2538, 2449, 1593, 546, 74, 3177, 2488, 3199, 1944, 3100, 2706, 1248, 2152, 1888, 2162, 880, 707, 118, 2838, 1370, 835, 3248, 1091, 1922, 178, 2765, 1642, 2050, 2101, 423, 711, 1363, 2564, 263, 1852, 2260, 2203, 730, 2711, 588, 354, 1285, 193, 5, 2741, 2605, 2603, 3013, 1343, 2825, 1038, 3330, 687, 2836, 2120, 108, 742, 803, 869, 1114, 3243, 2665, 2591, 1687, 110, 2792, 2918, 649, 1557, 3261, 1753, 794, 1919, 1070, 800, 1881, 496, 3156, 2068, 651, 1273, 1036, 481, 2522, 502, 2007, 574, 3304, 1928, 1238, 1889, 999, 3151, 2163, 2341, 1663, 2566, 2423, 1897, 150, 1083, 2853, 85, 799, 0, 2006, 3003, 3044, 2061, 227, 3223, 2799, 1969, 2743, 1201, 2912, 2110, 1397, 690, 702, 1855, 1138, 2020, 2271, 2179, 1001, 2421, 1910, 1573, 2338, 3062, 780, 192, 981, 2455, 3173, 2608, 2207, 1574, 3221, 238, 796, 2171, 335, 1647, 2130, 1786, 1422, 3026, 2217, 2215, 3220, 2880, 1271, 2794, 29, 2926, 266, 2858, 1916, 1313, 511, 3237, 975, 779, 2035, 3028, 724, 419, 2783, 920, 2443, 2371, 2673, 26, 1321, 737, 1227, 1194, 2049, 3291, 1368, 138, 2238, 1467, 115, 1525, 1451, 1289, 1954, 2916, 3107, 2898, 1514, 3180, 636, 2901, 83, 1223, 862, 1381, 647, 181, 2405, 1595, 1492, 2782, 899, 2637, 1931, 1661, 521, 3299, 367, 582, 3258, 2299, 1080, 1740, 627, 1968, 1067, 1706, 2977, 2553, 215, 3198, 2075, 1339, 71, 3121, 1119, 125, 2889, 120, 1214, 1215, 2279, 2195, 1746, 923, 59, 2402, 3023, 3300, 3006, 2450, 3120, 1317, 1409, 508, 205, 360, 2404, 898, 514, 40, 1104, 2013, 874, 2697, 3372, 830, 3205, 793, 352, 258, 572, 94, 2363, 1075, 372, 1820, 2316, 1666, 1469, 1304, 2625, 762, 296, 3285, 1173, 1133, 2177, 2407, 1630, 2139, 1042, 2081, 925, 2791, 2903, 847, 48, 2738, 2025, 3050, 105, 3249, 6, 1022, 2796, 479, 3086, 1658, 1960, 3090, 3174, 658, 2362, 95, 1659, 2546, 2659, 834, 628, 1233, 872, 480, 1826, 1506, 1633, 1802, 888, 3316, 622, 1324, 2694, 1775, 54, 2542, 449, 2624, 2144, 2733, 530, 1645, 2281, 2106, 1946, 854, 189, 1139, 812, 2798, 2690, 2302, 23, 494, 129, 2708, 63, 2689, 3200, 1503, 2235, 2243, 1305, 3025, 390, 1867, 2702, 727, 831, 829, 2159, 2684, 2172, 2283, 1668, 1731, 2397, 2248, 668, 2382, 1809, 2491, 2331, 435, 1695, 497, 2430, 2224, 2474, 1290, 384, 2014, 2501, 666, 695, 2976, 532, 820, 1189, 1117, 3166, 3141, 2067, 528, 1991, 659, 438, 341, 1724, 643, 1887, 1394, 1033, 2285, 2041, 816, 3265, 757, 3158, 2442, 1721, 2940, 1500, 300, 3184, 3381, 478, 2160, 1177, 602, 2655, 2364, 663, 662, 2319, 1134, 2307, 2343, 561, 950, 255, 3368, 2490, 3187, 2744, 556, 1517, 3183, 1008, 805, 2228, 2692, 3280, 3364, 1335, 3149, 1678, 597, 3085, 2498, 1172, 3338, 3127, 3109, 1442, 1589, 86, 564, 2517, 1027, 1804, 2259, 1697, 765, 2756, 585, 701, 1, 73, 3060, 1257, 1140, 3320, 1628, 3349, 2696, 2512, 2340, 1236, 823, 3247, 1801, 633, 450, 2077, 1069, 2748, 444, 2387, 2929, 2878, 452, 259, 679, 2908, 3363, 2146, 3169, 1736, 2389, 1078, 2718, 1974, 2329, 2122, 1213, 1518, 2759, 782, 1010, 1272, 422, 786, 198, 361, 1581, 2740, 581, 3269, 1196, 3345, 540, 1722, 462, 1031, 2473, 1037, 2310, 2230, 1452, 3322, 2444, 1102, 1406, 2620, 858, 817, 2070, 1224, 388, 2295, 2845, 1540, 1340, 463, 2859, 2820, 2439, 1959, 2424, 2005, 1703, 1089, 1605, 466, 1497, 3144, 1016, 571, 2828, 2826, 379, 915, 548, 2263, 485, 1167, 2214, 1839, 1636, 1461, 3088, 2985, 1726, 1192, 2127, 2019, 3352, 2857, 1019, 241, 980, 1463, 1456, 1615, 3070, 469, 2587, 3163, 1551, 2670, 1352, 1404, 1225, 1411, 2072, 554, 945, 1719, 1836, 2935, 1866, 304, 1296, 697, 673, 2115, 534, 2533, 2972, 1283, 2471, 2979, 1655, 234, 684, 2938, 3235, 3056, 1979, 1773, 130, 2209, 399, 3080, 2672, 3039, 338, 116, 1015, 2769, 2675, 1584, 2022, 340, 1254, 604, 2119, 2531, 568, 1987, 1212, 1264, 2018, 420, 2425, 610, 938, 3073, 505, 1592, 1891, 1681, 2393, 3234, 1094, 2652, 3024, 844, 2891, 303, 2647, 2398, 2763, 1239, 219, 2246, 1932, 166, 2735, 3239, 1745, 620, 2446, 1696, 380, 2816, 3042, 1121, 89, 2928, 1582, 2470, 216, 209, 2180, 1670, 2495, 982, 1873, 2460, 2520, 1446, 909, 1561, 832, 2016, 1314, 865, 2704, 897, 1798, 1197, 2919, 3045, 1842, 2457, 1132, 1813, 164, 594, 2255, 407, 2958, 2993, 3276, 3138, 3332, 1694, 172, 249, 183, 1905, 127, 1886, 3315, 2385, 3351, 605, 1649, 2071, 2987, 553, 2847, 1939, 1755, 3232, 2944, 1652, 1702, 2274, 21, 896, 2618, 1328, 2864, 2899, 1918, 1738, 3270, 842, 2688, 3328, 1311, 2126, 398, 808, 1603, 579, 212, 1816, 722, 2210, 3348, 3307, 1484, 1185, 906, 1493, 2459, 2932, 638, 2353, 2955, 12, 2651, 477, 2088, 2166, 887, 1827, 3192, 2896, 208, 1436, 3103, 1966, 507, 1221, 250, 1435, 838, 1524, 1505, 642, 1921, 2681, 1412, 2082, 1164, 603, 652, 1792, 2877, 621, 2960, 1964, 1504, 2167, 3240, 483, 537, 1757, 3196, 968, 1758, 1437, 3135, 2686, 3095, 676, 1077, 491, 3217, 11, 3146, 1149, 1112, 1818, 2900, 2187, 3301, 349, 18, 783, 436, 2392, 2663, 2881, 1228, 3010, 694, 7, 3370, 1560, 1103, 746, 934, 3289, 2266, 1784, 275, 2749, 343, 542, 2282, 3142, 1405, 1623, 370, 2586, 311, 1439, 357, 2888, 1426, 1539, 2311, 256, 1536, 947, 630, 1998, 1512, 1858, 640, 1190, 2805, 674, 2885, 721, 168, 392, 2360, 187, 1318, 1848, 580, 289, 1322, 596, 1198, 2284, 2504, 140, 50, 1519, 424, 2677, 1090, 1685, 1430, 525, 1612, 2669, 1136, 211, 2855, 165, 3188, 2722, 2352, 1030, 706, 156, 2909, 3030, 2876, 1768, 3185, 2256, 1817, 91, 1414, 1679, 1546, 774, 754, 1545, 1390, 2227, 2572, 639, 1307, 547, 268, 3082, 1578, 270, 123, 315, 484, 1346, 1220, 2793, 2410, 2322, 592, 2361, 245, 893, 1377, 2911, 2860, 1458, 455, 2556, 3342, 2465, 791, 856, 2760, 1677, 2807, 1182, 2497, 1606, 1028, 611, 242, 1181, 3091, 1174, 966, 1587, 415, 2923, 329, 3303, 1499, 1413, 617, 2514, 1914, 1260, 2479, 2357, 1298, 1877, 2551, 641, 2001, 244, 2438, 2986, 2234, 1129, 1608, 90, 1815, 1005, 1682, 3153, 2770, 2391, 1058, 1654, 428, 2927, 988, 2244, 2485, 618, 2674, 1614, 942, 958, 1478, 1071, 1926, 2326, 1186, 41, 2950, 713, 1883, 2108, 2806, 3182, 849, 905, 2775, 3134, 1391, 2486, 3078, 855, 2445, 1294, 2189, 2038, 201, 1098, 710, 3216, 3122, 1794, 1043, 1419, 8, 2480, 1532, 1594, 3273, 1126, 121, 644, 2121, 836, 1920, 3170, 2821, 1885, 3077, 1844, 2597, 1348, 2930, 1051, 3057, 2002, 784, 770, 769, 3019, 619, 3047, 2656, 1473, 33, 308, 1332, 1978, 1007, 3041, 332, 810, 1812, 153, 519, 93, 853, 886, 171, 2867, 1230, 1124, 2839, 2649, 1086, 328, 1674, 1199, 1195, 1060, 1002, 2784, 2593, 3257, 1814, 598, 1249, 2636, 2117, 1710, 1543, 2017, 2140, 2048, 2487, 3005, 1947, 2823, 682, 2990, 1997, 279, 1846, 1548, 204, 1331, 747, 2639, 3112, 1643, 1558, 1269, 2231, 1847, 1763, 859, 601, 3020, 826, 1534, 1143, 1896, 2309, 2724, 2345, 1680, 1231, 2602, 72, 1971, 1088, 1788, 1723, 522, 3167, 3309, 1369, 1996, 1247, 751, 2024, 2043, 3227, 2245, 2511, 903, 1531, 660, 1875, 1113, 545, 1849, 2968, 2946, 3022, 44, 112, 400, 1748, 1901, 1577, 1495, 3283, 1952, 2808, 1222, 2730, 3323, 3319, 1992, 2289, 3058, 623, 2532, 3308, 2118, 133, 2074, 102, 667, 471, 1443, 2601, 2489, 184, 2519, 3347, 1250, 1474, 1163, 889, 1904, 589, 2003, 1835, 2524, 1347, 612, 3037, 570, 151, 3129, 3350, 1356, 3087, 334, 1878, 64, 2612, 3253, 1320, 2031, 441, 3105, 3115, 2301, 185, 2496, 3094, 47, 1772, 1382, 3357, 2493, 88, 3252, 1750, 573, 3246, 281, 635, 1384, 2057, 1529, 1045, 2771, 1930, 2804, 3292, 265, 109, 3313, 3251, 2206, 1973, 2313, 2543, 2176, 373, 996, 292, 2713, 517, 3324, 1242, 2196, 1785, 2335, 2194, 1263, 1183, 2237, 1559, 1202, 2296, 2716, 1771, 2153, 3213, 1154, 1880, 2824, 2865, 821, 586, 80, 1488, 3191, 857, 3376, 1796, 1542, 1756, 509, 3101, 1243, 2337, 1764, 3089, 410, 39, 2330, 2609, 257, 919, 2056, 2635, 3354, 656, 1934, 2646, 2286, 2535, 2787, 1462, 2200, 3297, 2600, 3011, 1158, 374, 492, 2613, 1105, 569, 3356, 3190, 2592, 2178, 13, 595, 809, 14, 1207, 2844, 1470, 3255, 1278, 804, 900, 2942, 3033, 1638, 1629, 1845, 918, 1127, 3333, 1585, 188, 2441, 1020, 30, 3362, 225, 1166, 2948, 1170, 1541, 2135, 2317, 1993, 2561, 3084, 560, 529, 213, 1913, 3128, 3035, 1279, 1000, 36, 1156, 371, 2028, 2507, 1725, 2400, 2347, 2112, 1076, 2712, 1829, 467, 2190, 2879, 2666, 1787, 1619, 2358, 2009, 2150, 324, 1712, 2774, 3126, 700, 819, 1011, 1096, 421, 901, 761, 2643, 2634, 1571, 1936, 369, 96, 2606, 1893, 1565, 2395, 2789, 3334, 1762, 445, 3092, 318, 846, 1097, 549, 1733, 2023, 418, 2978, 273, 2872, 877, 3202, 412, 3195, 827, 2086, 1057, 131, 3225, 3310, 788, 1689, 218, 2435, 224, 1471, 1657, 624, 2419, 2089, 2500, 646, 262, 1556, 2842, 1970, 3004, 2988, 2272, 495, 356, 401, 3178, 3374, 2750, 1403, 2550, 176, 1791, 145, 1950, 37, 523, 382, 1065, 1150, 1933, 2373, 2526, 1316, 1523, 1253, 175, 2660, 1945, 1044, 243, 1165, 1082, 1387, 2569, 425, 3256, 1783, 501, 1599, 331, 161, 362, 1300, 921, 1728, 1553, 2211, 2472, 267, 2945, 2275, 655, 2411, 3139, 2776, 931, 3228, 2574, 2662, 1927, 1427, 3355, 868, 3081, 1282, 1562, 1912, 1393, 1537, 2137, 2802, 894, 1188, 1691, 1607, 2342, 773, 386, 2902, 2915, 1683, 3108, 248, 2406, 3076, 197, 2156, 1665, 973, 518, 2218, 2641, 2481, 316, 347, 1074, 1502, 2679, 1963, 297, 2113, 908, 802, 389, 3293, 1203, 1402, 2456, 1344, 2454, 1507, 2952, 965, 1860, 1554, 1152, 403, 2351, 1380, 27, 1770, 233, 190, 3099, 3136, 1509, 2980, 2063, 2621, 144, 2590, 498, 933, 1048, 927, 2850, 972, 475, 1361, 317, 426, 2133, 1211, 56, 1705, 38, 1145, 1859, 2974, 1464, 306, 881, 1776, 239, 448, 1717, 2094, 269, 2314, 2223, 1245, 1032, 1962, 3241, 1444, 2710, 696, 1206, 3272, 186, 634, 2381, 2866, 2107, 1162, 2714, 1237, 2212, 2109, 1874, 1800, 648, 1244, 1395, 2727, 1876, 3305, 34, 3336, 2982, 1735, 2085, 3179, 2293, 1781, 2452, 3008, 2267, 1226, 998, 3130, 2680, 1389, 1549, 504, 298, 1729, 2615, 3340, 1870, 1385, 427, 2241, 1365, 1204, 1868, 2193, 2970, 2871, 2440, 1739, 2339, 2000, 1479, 119, 203, 952, 2719, 2197, 629, 1994, 951, 2492, 1806, 1256, 2093, 261, 1081, 1341, 3380, 1455, 3193, 2966, 1334, 2379, 677, 814, 240, 1410, 2510, 194, 254, 2102, 3208, 3093, 2736, 845, 3377, 1564, 2396, 2562, 2142, 562, 2732, 755, 375, 1200, 689, 274, 3379, 1161, 2394, 1778, 3290, 302, 2614, 2494, 1417, 2065, 2567, 593, 813, 312, 405, 943, 734, 1972, 2910, 2409, 1440, 1486, 2188, 3137, 1379, 3119, 2715, 3160, 818, 1251, 971, 199, 1598, 3110, 1583, 3054, 3244, 1648, 3155, 778, 1106, 2922, 2463, 1235, 987, 2219, 510, 503, 1567, 1713, 489, 223, 3036, 2202, 625, 567, 3124, 961, 2701, 1808, 1650, 2432, 2557, 327, 252, 77, 1637, 2907, 1333, 1949, 2906, 1487, 87, 3302, 1496, 3157, 1988, 3373, 3219, 3002, 653, 104, 155, 2216, 1720, 1841, 1084, 637, 355, 3294, 2983, 1751, 714, 1692, 1447, 2033, 2873, 1906, 922, 807, 1366, 1782, 2242, 3067, 2205, 416, 2619, 2971, 789, 680, 174, 68, 2116, 1823, 60, 1258, 870, 2962, 246, 2015, 170, 3052, 348, 2788, 98, 1398, 2552, 2303, 251, 1407, 940, 2540, 1281, 2883, 2011, 979, 924, 2288, 1632, 2747, 1671, 2992, 1828, 3346, 795, 775, 447, 2664, 1900, 2981, 1759, 2728, 1899, 760, 1327, 608, 613, 217, 937, 3097, 3014, 2261, 2581, 17, 1766, 111, 913, 305, 2897, 1547, 1216, 2333, 985, 1441, 141, 1323, 32, 143, 1700, 541, 2098, 2627, 2174, 1276, 840, 2039, 2298, 1476, 2466, 2170, 3365, 31, 3189, 2386, 1769, 3329, 1291, 202, 2604, 3282, 2370, 2458, 2699, 708, 2164, 2964, 1662, 1869, 2328, 1653, 1137, 1209, 2525, 2273, 2226, 2623, 738, 2780, 1805, 322, 57, 278, 2811, 1831, 2051, 1286, 2408, 299, 607, 2645, 103, 3375, 2575, 2943, 2161, 2278, 2004, 2377, 3186, 1981, 2420, 2469, 977, 1151, 2374, 1109, 1308, 536, 1790, 2703, 693, 2506, 1135, 160, 55, 2058, 2073, 1265, 3366, 2698, 939, 2390, 914, 101, 20, 1942, 645, 891, 3051, 288, 2012, 935, 851, 2822, 1171, 1990, 1989, 2521, 2165, 1596, 2453, 122, 2434, 2268, 1570, 180, 1693, 1902, 1386, 2173, 2198, 1627, 1908, 2937, 506, 3, 1576, 3029, 863, 1040, 792, 954, 2729, 1364, 962, 309, 1984, 2059, 1148, 2995, 1099, 1765, 402, 173, 58, 3209, 2963, 1741, 3040, 2325, 2598, 928, 2251, 2843, 3268, 169, 2100, 2837, 1837, 2785, 3059, 2192, 2300, 3165, 2431, 2042, 1358, 2676, 2290, 2467, 1861, 2626, 716, 383, 1085, 2848, 413, 3145, 1374, 2097, 1445, 1618, 2813, 1911, 3116, 3281, 3222, 2233, 1457, 1302, 2306, 723, 1797, 2182, 2157, 2052, 1951, 433, 1092, 2447, 1023, 1141, 1610, 712, 3162, 1879, 2545, 377, 1193, 1854, 3321, 1580, 917, 1843, 2835, 177, 2893, 313, 1810, 1408, 2800, 1631, 2375, 2383, 196, 2476, 1591, 2415, 815, 1046, 353, 2292, 1999, 1982, 843, 2695, 1420, 3312, 142, 969, 1626, 3001, 2047, 9, 301, 1714, 1303, 76, 1059, 3369, 1012, 1354, 1466, 16, 768, 1675, 1777, 551, 616, 3066, 1280, 1115, 1634, 490, 226, 3152, 1651, 781, 2869, 2348, 1516, 1915, 3147, 806, 824, 672, 2734, 650, 465, 2709, 1246, 1351, 1315, 563, 2577, 2477, 1485, 1850, 2854, 404, 191, 1359, 1985, 1604, 895, 1353, 339, 1477, 990, 2247, 910, 2380, 2754, 878, 2642, 1644, 2726, 1004, 319, 2478, 1483, 2095, 731, 732, 1538, 2482, 237, 429, 70, 2658, 1144, 3009, 468, 685, 124, 3360, 748, 15, 453, 1853, 1360, 2607, 1822, 2725, 1660, 1701, 726, 2555, 1482, 2953, 3361, 2232, 3172, 1882, 956, 2904, 2818, 2921, 3074, 2417, 735, 1130, 1295, 1521, 1396, 159, 609, 3259, 3131, 99, 1563, 1862, 2044, 221, 512, 3306, 2815, 2851, 1673, 871, 2384, 2148, 1068, 3181, 575, 3210, 1625, 1372, 162, 2700, 739, 3245, 2327, 2104, 207, 615, 1475, 1948, 3250, 957, 3236, 42, 2947, 3083, 2829, 2403, 2571, 1063, 2693, 675, 2516, 460, 487, 1749, 2599, 670, 688, 3275, 2, 2132, 2451, 2611, 134, 1072]\n"
     ]
    }
   ],
   "source": [
    "TEST_PERCENT = 0.25\n",
    "\n",
    "# Load pgn paths\n",
    "pgns = import_data(5)\n",
    "\n",
    "# Convert pgns to tensors\n",
    "board_tensors, next_moves = parse_pgn_to_tensors(pgns)\n",
    "\n",
    "# Converting the dataset into a custom pytorch one\n",
    "dataset = ChessDataset(board_tensors, next_moves)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Splitting the data into train and test\n",
    "train_dataset, test_data = torch.utils.data.random_split(dataset, [1-TEST_PERCENT, TEST_PERCENT])\n",
    "\n",
    "print(len(test_data))  # Number of states\n",
    "print(train_dataset.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEURAL NETWORK DESIGN**\n",
    "- 2 Convolutional layers\n",
    "- 2 Fully connected hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to do the operations on the cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PieceToMoveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Takes as input a tensor of 14 channels (8x8 board)\n",
    "        self.conv1 = nn.Conv2d(14, 6, 3)  # 6 filters, 3x3 kernel\n",
    "        # self.pool = nn.MaxPool2d(2, 2)    # Max pooling with 2x2 window\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)  # 16 filters, 3x3 kernel\n",
    "\n",
    "        # Using droput to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        # Using batch normalization to make training faster and more stable\n",
    "        self.bn1 = nn.BatchNorm1d(240)  # For the 1st layer\n",
    "        self.bn2 = nn.BatchNorm1d(120)   # For the 2nd layer\n",
    "        \n",
    "        # Output from conv2 will be (16 channels, 1x1 feature maps)\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 240)\n",
    "        self.fc2 = nn.Linear(240, 120)\n",
    "        # Predicts the tile to move the piece from (64 possible tiles on the board)\n",
    "        self.fc3 = nn.Linear(120, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool(F.relu(self.conv1(x)))  # Apply first conv + pooling\n",
    "        x = F.reul(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))             # Apply second conv to get (16 x 1 x 1)\n",
    "        x = torch.flatten(x, 1)               # Flatten all dimensions except batch size\n",
    "        x = F.relu(self.bn1(self.fc1(x)))     # Fully connected layer 1 and batch normalization\n",
    "        x = self.dropout(x)                   # Dropout of some first layer neurons\n",
    "        x = F.relu(self.bn2(self.fc2(x)))     # Fully connected layer 2\n",
    "        x = self.fc3(x)                       # Output layer (no activation, logits for classification)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initializing the network\n",
    "piece_to_move_net = PieceToMoveNet()\n",
    "# Move the network to gpu/cpu befor initializing the optimizer\n",
    "piece_to_move_net.to(device)\n",
    "\n",
    "# Adam optimizer will be used due to its versatility\n",
    "optimizer = optim.Adam(piece_to_move_net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING LOOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(tensor)-> list:\n",
    "    \"\"\"Generates a mask which contains the position of the pieces that can move\"\"\"\n",
    "\n",
    "    # Contain a 2D representation of the board\n",
    "    # board_mask = torch.zeros((8,8))\n",
    "\n",
    "    # If layer 12 has any 1 it will be whites turn\n",
    "    if  torch.any(tensor[12] == 1):\n",
    "        # White pieces are in layers 0 to 5, apply a mask which will be 1 when there is a one\n",
    "        mask = torch.sum(tensor[0:6], dim = 0).int() # for summing across the layers\n",
    "\n",
    "\n",
    "    # If layer 13 has any 1 it will be blacks turn\n",
    "    elif torch.any(tensor[13] == 1):\n",
    "        # Black pieces are in range 6 to 11\n",
    "        # Apply a mask, if there is a piece it will be a 1\n",
    "        mask = torch.sum(tensor[6:12], dim = 0).int()\n",
    "\n",
    "    # A position wion zero value means a piece is present\n",
    "    # board_mask[mask > 0] = 1\n",
    "\n",
    "    # Flatten the board to a 1D array\n",
    "    # board_mask = board_mask.flatten().tolist()\n",
    "\n",
    "    return mask.flatten().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, loss_fn, train_sampler_size): \n",
    "    \"\"\"\n",
    "    Trains the model for one epoch and returns the average training loss and accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    running_loss = 0.  \n",
    "    running_correct = 0.\n",
    "\n",
    "    # Looping through all samples in a batch\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Extracting the board tensor\n",
    "        inputs = data[0]\n",
    "        # Extracting the tile of the piece to move\n",
    "        labels = data[1]\n",
    "\n",
    "        # Resetting the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculating model's output\n",
    "        mask = [generate_mask(pos) for pos in inputs]\n",
    "        mask = torch.tensor(mask)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        \"\"\"print(inputs[0])\n",
    "        print(labels[0])\n",
    "        print(mask[0])\"\"\"\n",
    "\n",
    "        # Moving inputs and labels to the gpu/cpu\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        outputs = logits * mask.float()\n",
    "\n",
    "        # Calculating the sample loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Calculating the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Updating model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adding the last loss to the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate number of correct predictions\n",
    "        _, predictions = torch.max(outputs.data, 1)\n",
    "        running_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    # Averaging the loss for all samples in the batch\n",
    "    running_loss /= (i + 1)\n",
    "\n",
    "    # Calculate accuracy based on the total samples in the fold (train_sampler_size)\n",
    "    train_accuracy = running_correct / train_sampler_size\n",
    "\n",
    "    return running_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validation_epoch(model, validation_loader, loss_fn, val_sampler_size):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch and returns the average validation loss and accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    running_vloss = 0.\n",
    "    running_vcorrect = 0.\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations for validation set\n",
    "    with torch.no_grad():\n",
    "        # Looping through all batches in the validation set\n",
    "        for i, v_data in enumerate(validation_loader):\n",
    "            # Getting the tensors of the validation data\n",
    "            vinputs = v_data[0]\n",
    "            vlabels = v_data[1] \n",
    "\n",
    "            # Calculating the output of the model\n",
    "            mask = [generate_mask(pos) for pos in vinputs]\n",
    "            mask = torch.tensor(mask)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            # Moving inputs and labels to the gpu/cpu\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "\n",
    "            logits = model(vinputs)\n",
    "            voutputs = logits * mask.float()\n",
    "\n",
    "            # Calculating the loss of the model in the validation sample\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            # Adding this sample's loss to the total loss\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "            # Calculate number of correct predictions\n",
    "            _, predictions = torch.max(voutputs.data, 1)\n",
    "            running_vcorrect += (predictions == vlabels).sum().item()\n",
    "\n",
    "    # Averaging the loss for all samples in the validation set\n",
    "    running_vloss /= (i + 1)\n",
    "\n",
    "    # Calculate accuracy based on the total samples in the fold (val_sampler_size)\n",
    "    validation_accuracy = running_vcorrect / val_sampler_size\n",
    "\n",
    "    return running_vloss, validation_accuracy\n",
    "\n",
    "\n",
    "def train_multiple_folds(n_epochs, n_folds, batch_size, splits, writer, optimizer_class, optimizer_params, loss_fn):\n",
    "\n",
    "    best_vloss = 1_000\n",
    "\n",
    "    epochs_tloss = [0 for _ in range(n_epochs)]\n",
    "    epochs_tacc = [0 for _ in range(n_epochs)]\n",
    "    epochs_vloss = [0 for _ in range(n_epochs)]\n",
    "    epochs_vacc = [0 for _ in range(n_epochs)]\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "        print(f\"FOLD {fold+1}\")\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        model = PieceToMoveNet()\n",
    "\n",
    "        optimizer = optimizer_class(model.parameters(), **optimizer_params)\n",
    "        model.to(device)\n",
    "\n",
    "        train_sampler_size = len(train_sampler)\n",
    "        val_sampler_size = len(val_sampler)\n",
    "        avg_vloss = 0.\n",
    "\n",
    "        for epoch in range(n_epochs): \n",
    "            train_loss, train_correct = train_epoch(model, optimizer, train_loader, loss_fn, train_sampler_size)\n",
    "            val_loss, val_correct = validation_epoch(model, val_loader, loss_fn, val_sampler_size)\n",
    "\n",
    "            avg_vloss += val_loss\n",
    "        \n",
    "            epochs_tloss[epoch] += train_loss\n",
    "            epochs_tacc[epoch] += train_correct\n",
    "            epochs_vloss[epoch] += val_loss\n",
    "            epochs_vacc[epoch] += val_correct\n",
    "\n",
    "            print(f\"Epoch: {epoch} Train Loss: {train_loss}, Valid Loss: {val_loss} | Train Acc: {train_correct}, Valid Acc: {val_correct}\")\n",
    "\n",
    "        avg_vloss /= (epoch + 1)\n",
    "\n",
    "        # Saving the model if the loss on the validation is lower than the best one\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"models/piece_to_move_net_{timestamp}_{fold}\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        epochs_tloss[i] /= (n_folds)\n",
    "        epochs_tacc[i] /= (n_folds)\n",
    "        epochs_vloss[i] /= (n_folds)\n",
    "        epochs_vacc[i] /= (n_folds)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # Adding insights\n",
    "        writer.add_scalars(\"Loss\", {\"Training\": epochs_tloss[i], \"Validation\": epochs_vloss[i]}, i + 1)\n",
    "        writer.add_scalars(\"Accuracy\", {\"Training\": epochs_tacc[i], \"Validation\": epochs_vacc[i]}, i + 1)\n",
    "        writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Epoch: 0 Train Loss: 4.110798169981758, Valid Loss: 4.131084865993923 | Train Acc: 0.10999408633944412, Valid Acc: 0.0851063829787234\n",
      "Epoch: 1 Train Loss: 4.054035785063258, Valid Loss: 3.999702533086141 | Train Acc: 0.11945594322885866, Valid Acc: 0.12293144208037825\n",
      "Epoch: 2 Train Loss: 3.87433042166368, Valid Loss: 3.788986320848818 | Train Acc: 0.15493790656416323, Valid Acc: 0.1453900709219858\n",
      "Epoch: 3 Train Loss: 3.5317449254809685, Valid Loss: 3.3594006609033653 | Train Acc: 0.17031342400946187, Valid Acc: 0.17257683215130024\n",
      "Epoch: 4 Train Loss: 3.0377993223802098, Valid Loss: 2.89467273818122 | Train Acc: 0.1862803075103489, Valid Acc: 0.18085106382978725\n",
      "Epoch: 5 Train Loss: 2.6316502679069087, Valid Loss: 2.54697377593429 | Train Acc: 0.19396806623299823, Valid Acc: 0.18439716312056736\n",
      "Epoch: 6 Train Loss: 2.407414305884883, Valid Loss: 2.406557701252125 | Train Acc: 0.20697811945594322, Valid Acc: 0.20212765957446807\n",
      "Epoch: 7 Train Loss: 2.312703146124786, Valid Loss: 2.3536223305596247 | Train Acc: 0.19751626256652868, Valid Acc: 0.20212765957446807\n",
      "Epoch: 8 Train Loss: 2.2608149501512633, Valid Loss: 2.3295741699360035 | Train Acc: 0.21289178001182732, Valid Acc: 0.18557919621749408\n",
      "Epoch: 9 Train Loss: 2.2317356208585344, Valid Loss: 2.3127626754619457 | Train Acc: 0.21880544056771142, Valid Acc: 0.1879432624113475\n",
      "Epoch: 10 Train Loss: 2.212031321705512, Valid Loss: 2.2936862486380116 | Train Acc: 0.231224127735068, Valid Acc: 0.2033096926713948\n",
      "Epoch: 11 Train Loss: 2.2023468579886094, Valid Loss: 2.2924442202956588 | Train Acc: 0.2253104671791839, Valid Acc: 0.1867612293144208\n",
      "Epoch: 12 Train Loss: 2.179700626517242, Valid Loss: 2.293583375436288 | Train Acc: 0.244234180958013, Valid Acc: 0.1867612293144208\n",
      "Epoch: 13 Train Loss: 2.174599143694032, Valid Loss: 2.2797152642850524 | Train Acc: 0.23595505617977527, Valid Acc: 0.18557919621749408\n",
      "Epoch: 14 Train Loss: 2.15262559899744, Valid Loss: 2.2833720136571816 | Train Acc: 0.2489651094027203, Valid Acc: 0.19621749408983452\n",
      "Epoch: 15 Train Loss: 2.149404465027575, Valid Loss: 2.271693061899256 | Train Acc: 0.2495564754583087, Valid Acc: 0.19267139479905437\n",
      "Epoch: 16 Train Loss: 2.1356052430170887, Valid Loss: 2.280689036404645 | Train Acc: 0.25310467179183915, Valid Acc: 0.20921985815602837\n",
      "Epoch: 17 Train Loss: 2.1301766723956703, Valid Loss: 2.2721123430464 | Train Acc: 0.26079243051448847, Valid Acc: 0.19739952718676124\n",
      "Epoch: 18 Train Loss: 2.115625932531537, Valid Loss: 2.2640177055641457 | Train Acc: 0.26552335895919577, Valid Acc: 0.19739952718676124\n",
      "Epoch: 19 Train Loss: 2.101658542201204, Valid Loss: 2.2697245324099504 | Train Acc: 0.2643406268480189, Valid Acc: 0.19739952718676124\n",
      "Epoch: 20 Train Loss: 2.0942361647228025, Valid Loss: 2.269331207981816 | Train Acc: 0.2767593140153755, Valid Acc: 0.19621749408983452\n",
      "Epoch: 21 Train Loss: 2.0823362503411635, Valid Loss: 2.258418180324413 | Train Acc: 0.28444707273802483, Valid Acc: 0.19739952718676124\n",
      "Epoch: 22 Train Loss: 2.0755280953533246, Valid Loss: 2.260696534757261 | Train Acc: 0.28503843879361324, Valid Acc: 0.2115839243498818\n",
      "Epoch: 23 Train Loss: 2.0683368736842893, Valid Loss: 2.2842093573676214 | Train Acc: 0.2820816085156712, Valid Acc: 0.19030732860520094\n",
      "Epoch: 24 Train Loss: 2.0584466277428395, Valid Loss: 2.270279539955987 | Train Acc: 0.28503843879361324, Valid Acc: 0.20094562647754138\n",
      "FOLD 2\n",
      "Epoch: 0 Train Loss: 4.146021244660863, Valid Loss: 4.134054519512035 | Train Acc: 0.10999408633944412, Valid Acc: 0.11465721040189125\n",
      "Epoch: 1 Train Loss: 4.047654066445692, Valid Loss: 3.965472353829278 | Train Acc: 0.140153755174453, Valid Acc: 0.1276595744680851\n",
      "Epoch: 2 Train Loss: 3.787101493691498, Valid Loss: 3.6124664500907615 | Train Acc: 0.14606741573033707, Valid Acc: 0.12411347517730496\n",
      "Epoch: 3 Train Loss: 3.300179562478695, Valid Loss: 3.0870737852873624 | Train Acc: 0.15257244234180958, Valid Acc: 0.13120567375886524\n",
      "Epoch: 4 Train Loss: 2.8250156573529512, Valid Loss: 2.706490543153551 | Train Acc: 0.16794795978710822, Valid Acc: 0.1690307328605201\n",
      "Epoch: 5 Train Loss: 2.534865338847322, Valid Loss: 2.477538753438879 | Train Acc: 0.17386162034299232, Valid Acc: 0.20212765957446807\n",
      "Epoch: 6 Train Loss: 2.394744283748123, Valid Loss: 2.3978133113295943 | Train Acc: 0.18864577173270256, Valid Acc: 0.19976359338061467\n",
      "Epoch: 7 Train Loss: 2.3311720344255553, Valid Loss: 2.371658369346901 | Train Acc: 0.18746303962152572, Valid Acc: 0.19148936170212766\n",
      "Epoch: 8 Train Loss: 2.293388402686929, Valid Loss: 2.3401554778770164 | Train Acc: 0.192193968066233, Valid Acc: 0.19739952718676124\n",
      "Epoch: 9 Train Loss: 2.2760151602187246, Valid Loss: 2.327725984432079 | Train Acc: 0.19455943228858663, Valid Acc: 0.19267139479905437\n",
      "Epoch: 10 Train Loss: 2.257449782119607, Valid Loss: 2.3227091277087175 | Train Acc: 0.20697811945594322, Valid Acc: 0.19621749408983452\n",
      "Epoch: 11 Train Loss: 2.2453910832135184, Valid Loss: 2.3049344221750894 | Train Acc: 0.21052631578947367, Valid Acc: 0.19858156028368795\n",
      "Epoch: 12 Train Loss: 2.236950689891599, Valid Loss: 2.300121298542729 | Train Acc: 0.20283855706682435, Valid Acc: 0.1867612293144208\n",
      "Epoch: 13 Train Loss: 2.2228811911816866, Valid Loss: 2.2973394835436785 | Train Acc: 0.21880544056771142, Valid Acc: 0.19739952718676124\n",
      "Epoch: 14 Train Loss: 2.218336768870084, Valid Loss: 2.2991792449244746 | Train Acc: 0.21762270845653459, Valid Acc: 0.1690307328605201\n",
      "Epoch: 15 Train Loss: 2.2033876180648804, Valid Loss: 2.281107302065249 | Train Acc: 0.2235363690124187, Valid Acc: 0.20685579196217493\n",
      "Epoch: 16 Train Loss: 2.195021629333496, Valid Loss: 2.2924442644472474 | Train Acc: 0.22176227084565345, Valid Acc: 0.1867612293144208\n",
      "Epoch: 17 Train Loss: 2.1887886569185078, Valid Loss: 2.277963669211776 | Train Acc: 0.23891188645771733, Valid Acc: 0.19858156028368795\n",
      "Epoch: 18 Train Loss: 2.1795004493785353, Valid Loss: 2.282354752222697 | Train Acc: 0.23536369012418687, Valid Acc: 0.1938534278959811\n",
      "Epoch: 19 Train Loss: 2.1651797024708874, Valid Loss: 2.2803826950214527 | Train Acc: 0.2436428149024246, Valid Acc: 0.19739952718676124\n",
      "Epoch: 20 Train Loss: 2.1598762791111783, Valid Loss: 2.2876404523849487 | Train Acc: 0.23536369012418687, Valid Acc: 0.19030732860520094\n",
      "Epoch: 21 Train Loss: 2.1522772739518365, Valid Loss: 2.275509383943346 | Train Acc: 0.25428740390301596, Valid Acc: 0.1938534278959811\n",
      "Epoch: 22 Train Loss: 2.140400911277195, Valid Loss: 2.272402224717317 | Train Acc: 0.25428740390301596, Valid Acc: 0.2033096926713948\n",
      "Epoch: 23 Train Loss: 2.1323758836062447, Valid Loss: 2.255581498146057 | Train Acc: 0.25428740390301596, Valid Acc: 0.2044917257683215\n",
      "Epoch: 24 Train Loss: 2.125271354081496, Valid Loss: 2.2623610805582115 | Train Acc: 0.25960969840331166, Valid Acc: 0.20212765957446807\n",
      "FOLD 3\n",
      "Epoch: 0 Train Loss: 4.1120058230634005, Valid Loss: 4.102123525407579 | Train Acc: 0.12293144208037825, Valid Acc: 0.09230769230769231\n",
      "Epoch: 1 Train Loss: 4.02898362897477, Valid Loss: 3.9504619439442954 | Train Acc: 0.12293144208037825, Valid Acc: 0.1289940828402367\n",
      "Epoch: 2 Train Loss: 3.813392495209316, Valid Loss: 3.678557872772217 | Train Acc: 0.1400709219858156, Valid Acc: 0.13609467455621302\n",
      "Epoch: 3 Train Loss: 3.4392535371600457, Valid Loss: 3.2355231444040933 | Train Acc: 0.15780141843971632, Valid Acc: 0.13136094674556212\n",
      "Epoch: 4 Train Loss: 2.9711184231740124, Valid Loss: 2.824519846174452 | Train Acc: 0.17966903073286053, Valid Acc: 0.15621301775147928\n",
      "Epoch: 5 Train Loss: 2.626544767955564, Valid Loss: 2.605025397406684 | Train Acc: 0.18203309692671396, Valid Acc: 0.16568047337278108\n",
      "Epoch: 6 Train Loss: 2.439454389068316, Valid Loss: 2.4727750884162054 | Train Acc: 0.19562647754137116, Valid Acc: 0.17633136094674556\n",
      "Epoch: 7 Train Loss: 2.349742974875108, Valid Loss: 2.4291611159289324 | Train Acc: 0.21631205673758866, Valid Acc: 0.15857988165680473\n",
      "Epoch: 8 Train Loss: 2.305126860456647, Valid Loss: 2.3778217015443026 | Train Acc: 0.2186761229314421, Valid Acc: 0.1621301775147929\n",
      "Epoch: 9 Train Loss: 2.2788219496888935, Valid Loss: 2.3453179288793495 | Train Acc: 0.21217494089834515, Valid Acc: 0.1668639053254438\n",
      "Epoch: 10 Train Loss: 2.255931507866338, Valid Loss: 2.3254841257024697 | Train Acc: 0.20685579196217493, Valid Acc: 0.1680473372781065\n",
      "Epoch: 11 Train Loss: 2.240431585401859, Valid Loss: 2.316694498062134 | Train Acc: 0.21513002364066194, Valid Acc: 0.16094674556213018\n",
      "Epoch: 12 Train Loss: 2.223782179490575, Valid Loss: 2.3074008535455772 | Train Acc: 0.23699763593380616, Valid Acc: 0.15739644970414202\n",
      "Epoch: 13 Train Loss: 2.214389607591449, Valid Loss: 2.3034715387556286 | Train Acc: 0.21040189125295508, Valid Acc: 0.1668639053254438\n",
      "Epoch: 14 Train Loss: 2.2069103357926854, Valid Loss: 2.297303835550944 | Train Acc: 0.2281323877068558, Valid Acc: 0.15976331360946747\n",
      "Epoch: 15 Train Loss: 2.198891950103472, Valid Loss: 2.2877274089389377 | Train Acc: 0.22044917257683216, Valid Acc: 0.16094674556213018\n",
      "Epoch: 16 Train Loss: 2.188920068290998, Valid Loss: 2.280666360148677 | Train Acc: 0.23522458628841608, Valid Acc: 0.17514792899408285\n",
      "Epoch: 17 Train Loss: 2.175985815390101, Valid Loss: 2.2867143860569707 | Train Acc: 0.2364066193853428, Valid Acc: 0.1668639053254438\n",
      "Epoch: 18 Train Loss: 2.1718436951907174, Valid Loss: 2.267708654756899 | Train Acc: 0.24054373522458627, Valid Acc: 0.178698224852071\n",
      "Epoch: 19 Train Loss: 2.1634637567232238, Valid Loss: 2.2777567174699573 | Train Acc: 0.24231678486997635, Valid Acc: 0.18106508875739644\n",
      "Epoch: 20 Train Loss: 2.1549486709090897, Valid Loss: 2.27339110551057 | Train Acc: 0.24940898345153664, Valid Acc: 0.18461538461538463\n",
      "Epoch: 21 Train Loss: 2.144540145712079, Valid Loss: 2.266130535690873 | Train Acc: 0.24468085106382978, Valid Acc: 0.18579881656804734\n",
      "Epoch: 22 Train Loss: 2.1325616184270606, Valid Loss: 2.2523684766557484 | Train Acc: 0.26004728132387706, Valid Acc: 0.18698224852071005\n",
      "Epoch: 23 Train Loss: 2.12028033103583, Valid Loss: 2.264903112694069 | Train Acc: 0.27364066193853426, Valid Acc: 0.17751479289940827\n",
      "Epoch: 24 Train Loss: 2.113314646594929, Valid Loss: 2.2683103349473743 | Train Acc: 0.26182033096926716, Valid Acc: 0.19053254437869824\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "K = 3\n",
    "# Logs training statistics for TensorBoard visualization\n",
    "writer = SummaryWriter(f\"runs/piece_to_move_{timestamp}\")  \n",
    "splits = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "optimizer_class = optim.Adam\n",
    "optimizer_params = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-5\n",
    "}\n",
    "\n",
    "train_multiple_folds(EPOCHS, K, BATCH_SIZE, splits, writer, optimizer_class, optimizer_params, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
