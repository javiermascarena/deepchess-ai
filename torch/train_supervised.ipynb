{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aux_functions\n",
    "import importlib\n",
    "\n",
    "importlib.reload(aux_functions)\n",
    "from aux_functions import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "#from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import chess\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PROCESSING**\n",
    "\n",
    "- Importing the pgn data\n",
    "- Transforming the data to sparce tensors \n",
    "- Splitting the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845\n",
      "[414, 235, 1355, 1009, 437, 1064, 2105, 260, 2175, 81, 291, 79, 728, 2653, 2508, 1510, 2518, 2965, 2428, 1621, 2124, 763, 1293, 797, 1454, 1299, 443, 686, 967, 2616, 231, 283, 2045, 49, 2573, 717, 2412, 2931, 1205, 376, 678, 3326, 2258, 986, 1175, 861, 929, 1622, 3254, 3260, 3038, 408, 705, 1465, 949, 1752, 3331, 3113, 222, 365, 879, 128, 3065, 1309, 1241, 3233, 284, 2849, 2422, 2786, 1131, 1511, 92, 1568, 114, 2332, 2917, 1498, 1287, 1329, 126, 2037, 1326, 1383, 995, 1641, 3007, 3150, 293, 671, 2797, 206, 974, 3096, 2249, 229, 2462, 4, 2707, 1108, 35, 1747, 3098, 3055, 442, 1006, 2830, 2053, 393, 2667, 2633, 883, 1056, 1983, 1274, 1399, 3317, 100, 2969, 1744, 3337, 62, 1575, 2861, 1433, 715, 719, 2141, 2795, 2809, 446, 2584, 2959, 2831, 470, 1093, 2034, 381, 2264, 1448, 683, 1388, 1432, 1123, 1047, 941, 2257, 976, 959, 699, 790, 767, 2064, 2994, 2568, 2367, 833, 3168, 1400, 1325, 2240, 277, 2949, 1449, 2513, 3061, 1336, 1401, 3335, 290, 3339, 1367, 2817, 3114, 1415, 787, 2087, 2304, 464, 1535, 3207, 1555, 321, 2790, 2503, 1856, 1438, 2145, 3262, 2638, 1833, 157, 378, 2761, 1734, 1345, 1530, 2149, 2868, 113, 997, 1424, 2737, 2368, 2280, 1995, 1803, 1871, 2538, 2449, 1593, 546, 74, 3177, 2488, 3199, 1944, 3100, 2706, 1248, 2152, 1888, 2162, 880, 707, 118, 2838, 1370, 835, 3248, 1091, 1922, 178, 2765, 1642, 2050, 2101, 423, 711, 1363, 2564, 263, 1852, 2260, 2203, 730, 2711, 588, 354, 1285, 193, 5, 2741, 2605, 2603, 3013, 1343, 2825, 1038, 3330, 687, 2836, 2120, 108, 742, 803, 869, 1114, 3243, 2665, 2591, 1687, 110, 2792, 2918, 649, 1557, 3261, 1753, 794, 1919, 1070, 800, 1881, 496, 3156, 2068, 651, 1273, 1036, 481, 2522, 502, 2007, 574, 3304, 1928, 1238, 1889, 999, 3151, 2163, 2341, 1663, 2566, 2423, 1897, 150, 1083, 2853, 85, 799, 0, 2006, 3003, 3044, 2061, 227, 3223, 2799, 1969, 2743, 1201, 2912, 2110, 1397, 690, 702, 1855, 1138, 2020, 2271, 2179, 1001, 2421, 1910, 1573, 2338, 3062, 780, 192, 981, 2455, 3173, 2608, 2207, 1574, 3221, 238, 796, 2171, 335, 1647, 2130, 1786, 1422, 3026, 2217, 2215, 3220, 2880, 1271, 2794, 29, 2926, 266, 2858, 1916, 1313, 511, 3237, 975, 779, 2035, 3028, 724, 419, 2783, 920, 2443, 2371, 2673, 26, 1321, 737, 1227, 1194, 2049, 3291, 1368, 138, 2238, 1467, 115, 1525, 1451, 1289, 1954, 2916, 3107, 2898, 1514, 3180, 636, 2901, 83, 1223, 862, 1381, 647, 181, 2405, 1595, 1492, 2782, 899, 2637, 1931, 1661, 521, 3299, 367, 582, 3258, 2299, 1080, 1740, 627, 1968, 1067, 1706, 2977, 2553, 215, 3198, 2075, 1339, 71, 3121, 1119, 125, 2889, 120, 1214, 1215, 2279, 2195, 1746, 923, 59, 2402, 3023, 3300, 3006, 2450, 3120, 1317, 1409, 508, 205, 360, 2404, 898, 514, 40, 1104, 2013, 874, 2697, 3372, 830, 3205, 793, 352, 258, 572, 94, 2363, 1075, 372, 1820, 2316, 1666, 1469, 1304, 2625, 762, 296, 3285, 1173, 1133, 2177, 2407, 1630, 2139, 1042, 2081, 925, 2791, 2903, 847, 48, 2738, 2025, 3050, 105, 3249, 6, 1022, 2796, 479, 3086, 1658, 1960, 3090, 3174, 658, 2362, 95, 1659, 2546, 2659, 834, 628, 1233, 872, 480, 1826, 1506, 1633, 1802, 888, 3316, 622, 1324, 2694, 1775, 54, 2542, 449, 2624, 2144, 2733, 530, 1645, 2281, 2106, 1946, 854, 189, 1139, 812, 2798, 2690, 2302, 23, 494, 129, 2708, 63, 2689, 3200, 1503, 2235, 2243, 1305, 3025, 390, 1867, 2702, 727, 831, 829, 2159, 2684, 2172, 2283, 1668, 1731, 2397, 2248, 668, 2382, 1809, 2491, 2331, 435, 1695, 497, 2430, 2224, 2474, 1290, 384, 2014, 2501, 666, 695, 2976, 532, 820, 1189, 1117, 3166, 3141, 2067, 528, 1991, 659, 438, 341, 1724, 643, 1887, 1394, 1033, 2285, 2041, 816, 3265, 757, 3158, 2442, 1721, 2940, 1500, 300, 3184, 3381, 478, 2160, 1177, 602, 2655, 2364, 663, 662, 2319, 1134, 2307, 2343, 561, 950, 255, 3368, 2490, 3187, 2744, 556, 1517, 3183, 1008, 805, 2228, 2692, 3280, 3364, 1335, 3149, 1678, 597, 3085, 2498, 1172, 3338, 3127, 3109, 1442, 1589, 86, 564, 2517, 1027, 1804, 2259, 1697, 765, 2756, 585, 701, 1, 73, 3060, 1257, 1140, 3320, 1628, 3349, 2696, 2512, 2340, 1236, 823, 3247, 1801, 633, 450, 2077, 1069, 2748, 444, 2387, 2929, 2878, 452, 259, 679, 2908, 3363, 2146, 3169, 1736, 2389, 1078, 2718, 1974, 2329, 2122, 1213, 1518, 2759, 782, 1010, 1272, 422, 786, 198, 361, 1581, 2740, 581, 3269, 1196, 3345, 540, 1722, 462, 1031, 2473, 1037, 2310, 2230, 1452, 3322, 2444, 1102, 1406, 2620, 858, 817, 2070, 1224, 388, 2295, 2845, 1540, 1340, 463, 2859, 2820, 2439, 1959, 2424, 2005, 1703, 1089, 1605, 466, 1497, 3144, 1016, 571, 2828, 2826, 379, 915, 548, 2263, 485, 1167, 2214, 1839, 1636, 1461, 3088, 2985, 1726, 1192, 2127, 2019, 3352, 2857, 1019, 241, 980, 1463, 1456, 1615, 3070, 469, 2587, 3163, 1551, 2670, 1352, 1404, 1225, 1411, 2072, 554, 945, 1719, 1836, 2935, 1866, 304, 1296, 697, 673, 2115, 534, 2533, 2972, 1283, 2471, 2979, 1655, 234, 684, 2938, 3235, 3056, 1979, 1773, 130, 2209, 399, 3080, 2672, 3039, 338, 116, 1015, 2769, 2675, 1584, 2022, 340, 1254, 604, 2119, 2531, 568, 1987, 1212, 1264, 2018, 420, 2425, 610, 938, 3073, 505, 1592, 1891, 1681, 2393, 3234, 1094, 2652, 3024, 844, 2891, 303, 2647, 2398, 2763, 1239, 219, 2246, 1932, 166, 2735, 3239, 1745, 620, 2446, 1696, 380, 2816, 3042, 1121, 89, 2928, 1582, 2470, 216, 209, 2180, 1670, 2495, 982, 1873, 2460, 2520, 1446, 909, 1561, 832, 2016, 1314, 865, 2704, 897, 1798, 1197, 2919, 3045, 1842, 2457, 1132, 1813, 164, 594, 2255, 407, 2958, 2993, 3276, 3138, 3332, 1694, 172, 249, 183, 1905, 127, 1886, 3315, 2385, 3351, 605, 1649, 2071, 2987, 553, 2847, 1939, 1755, 3232, 2944, 1652, 1702, 2274, 21, 896, 2618, 1328, 2864, 2899, 1918, 1738, 3270, 842, 2688, 3328, 1311, 2126, 398, 808, 1603, 579, 212, 1816, 722, 2210, 3348, 3307, 1484, 1185, 906, 1493, 2459, 2932, 638, 2353, 2955, 12, 2651, 477, 2088, 2166, 887, 1827, 3192, 2896, 208, 1436, 3103, 1966, 507, 1221, 250, 1435, 838, 1524, 1505, 642, 1921, 2681, 1412, 2082, 1164, 603, 652, 1792, 2877, 621, 2960, 1964, 1504, 2167, 3240, 483, 537, 1757, 3196, 968, 1758, 1437, 3135, 2686, 3095, 676, 1077, 491, 3217, 11, 3146, 1149, 1112, 1818, 2900, 2187, 3301, 349, 18, 783, 436, 2392, 2663, 2881, 1228, 3010, 694, 7, 3370, 1560, 1103, 746, 934, 3289, 2266, 1784, 275, 2749, 343, 542, 2282, 3142, 1405, 1623, 370, 2586, 311, 1439, 357, 2888, 1426, 1539, 2311, 256, 1536, 947, 630, 1998, 1512, 1858, 640, 1190, 2805, 674, 2885, 721, 168, 392, 2360, 187, 1318, 1848, 580, 289, 1322, 596, 1198, 2284, 2504, 140, 50, 1519, 424, 2677, 1090, 1685, 1430, 525, 1612, 2669, 1136, 211, 2855, 165, 3188, 2722, 2352, 1030, 706, 156, 2909, 3030, 2876, 1768, 3185, 2256, 1817, 91, 1414, 1679, 1546, 774, 754, 1545, 1390, 2227, 2572, 639, 1307, 547, 268, 3082, 1578, 270, 123, 315, 484, 1346, 1220, 2793, 2410, 2322, 592, 2361, 245, 893, 1377, 2911, 2860, 1458, 455, 2556, 3342, 2465, 791, 856, 2760, 1677, 2807, 1182, 2497, 1606, 1028, 611, 242, 1181, 3091, 1174, 966, 1587, 415, 2923, 329, 3303, 1499, 1413, 617, 2514, 1914, 1260, 2479, 2357, 1298, 1877, 2551, 641, 2001, 244, 2438, 2986, 2234, 1129, 1608, 90, 1815, 1005, 1682, 3153, 2770, 2391, 1058, 1654, 428, 2927, 988, 2244, 2485, 618, 2674, 1614, 942, 958, 1478, 1071, 1926, 2326, 1186, 41, 2950, 713, 1883, 2108, 2806, 3182, 849, 905, 2775, 3134, 1391, 2486, 3078, 855, 2445, 1294, 2189, 2038, 201, 1098, 710, 3216, 3122, 1794, 1043, 1419, 8, 2480, 1532, 1594, 3273, 1126, 121, 644, 2121, 836, 1920, 3170, 2821, 1885, 3077, 1844, 2597, 1348, 2930, 1051, 3057, 2002, 784, 770, 769, 3019, 619, 3047, 2656, 1473, 33, 308, 1332, 1978, 1007, 3041, 332, 810, 1812, 153, 519, 93, 853, 886, 171, 2867, 1230, 1124, 2839, 2649, 1086, 328, 1674, 1199, 1195, 1060, 1002, 2784, 2593, 3257, 1814, 598, 1249, 2636, 2117, 1710, 1543, 2017, 2140, 2048, 2487, 3005, 1947, 2823, 682, 2990, 1997, 279, 1846, 1548, 204, 1331, 747, 2639, 3112, 1643, 1558, 1269, 2231, 1847, 1763, 859, 601, 3020, 826, 1534, 1143, 1896, 2309, 2724, 2345, 1680, 1231, 2602, 72, 1971, 1088, 1788, 1723, 522, 3167, 3309, 1369, 1996, 1247, 751, 2024, 2043, 3227, 2245, 2511, 903, 1531, 660, 1875, 1113, 545, 1849, 2968, 2946, 3022, 44, 112, 400, 1748, 1901, 1577, 1495, 3283, 1952, 2808, 1222, 2730, 3323, 3319, 1992, 2289, 3058, 623, 2532, 3308, 2118, 133, 2074, 102, 667, 471, 1443, 2601, 2489, 184, 2519, 3347, 1250, 1474, 1163, 889, 1904, 589, 2003, 1835, 2524, 1347, 612, 3037, 570, 151, 3129, 3350, 1356, 3087, 334, 1878, 64, 2612, 3253, 1320, 2031, 441, 3105, 3115, 2301, 185, 2496, 3094, 47, 1772, 1382, 3357, 2493, 88, 3252, 1750, 573, 3246, 281, 635, 1384, 2057, 1529, 1045, 2771, 1930, 2804, 3292, 265, 109, 3313, 3251, 2206, 1973, 2313, 2543, 2176, 373, 996, 292, 2713, 517, 3324, 1242, 2196, 1785, 2335, 2194, 1263, 1183, 2237, 1559, 1202, 2296, 2716, 1771, 2153, 3213, 1154, 1880, 2824, 2865, 821, 586, 80, 1488, 3191, 857, 3376, 1796, 1542, 1756, 509, 3101, 1243, 2337, 1764, 3089, 410, 39, 2330, 2609, 257, 919, 2056, 2635, 3354, 656, 1934, 2646, 2286, 2535, 2787, 1462, 2200, 3297, 2600, 3011, 1158, 374, 492, 2613, 1105, 569, 3356, 3190, 2592, 2178, 13, 595, 809, 14, 1207, 2844, 1470, 3255, 1278, 804, 900, 2942, 3033, 1638, 1629, 1845, 918, 1127, 3333, 1585, 188, 2441, 1020, 30, 3362, 225, 1166, 2948, 1170, 1541, 2135, 2317, 1993, 2561, 3084, 560, 529, 213, 1913, 3128, 3035, 1279, 1000, 36, 1156, 371, 2028, 2507, 1725, 2400, 2347, 2112, 1076, 2712, 1829, 467, 2190, 2879, 2666, 1787, 1619, 2358, 2009, 2150, 324, 1712, 2774, 3126, 700, 819, 1011, 1096, 421, 901, 761, 2643, 2634, 1571, 1936, 369, 96, 2606, 1893, 1565, 2395, 2789, 3334, 1762, 445, 3092, 318, 846, 1097, 549, 1733, 2023, 418, 2978, 273, 2872, 877, 3202, 412, 3195, 827, 2086, 1057, 131, 3225, 3310, 788, 1689, 218, 2435, 224, 1471, 1657, 624, 2419, 2089, 2500, 646, 262, 1556, 2842, 1970, 3004, 2988, 2272, 495, 356, 401, 3178, 3374, 2750, 1403, 2550, 176, 1791, 145, 1950, 37, 523, 382, 1065, 1150, 1933, 2373, 2526, 1316, 1523, 1253, 175, 2660, 1945, 1044, 243, 1165, 1082, 1387, 2569, 425, 3256, 1783, 501, 1599, 331, 161, 362, 1300, 921, 1728, 1553, 2211, 2472, 267, 2945, 2275, 655, 2411, 3139, 2776, 931, 3228, 2574, 2662, 1927, 1427, 3355, 868, 3081, 1282, 1562, 1912, 1393, 1537, 2137, 2802, 894, 1188, 1691, 1607, 2342, 773, 386, 2902, 2915, 1683, 3108, 248, 2406, 3076, 197, 2156, 1665, 973, 518, 2218, 2641, 2481, 316, 347, 1074, 1502, 2679, 1963, 297, 2113, 908, 802, 389, 3293, 1203, 1402, 2456, 1344, 2454, 1507, 2952, 965, 1860, 1554, 1152, 403, 2351, 1380, 27, 1770, 233, 190, 3099, 3136, 1509, 2980, 2063, 2621, 144, 2590, 498, 933, 1048, 927, 2850, 972, 475, 1361, 317, 426, 2133, 1211, 56, 1705, 38, 1145, 1859, 2974, 1464, 306, 881, 1776, 239, 448, 1717, 2094, 269, 2314, 2223, 1245, 1032, 1962, 3241, 1444, 2710, 696, 1206, 3272, 186, 634, 2381, 2866, 2107, 1162, 2714, 1237, 2212, 2109, 1874, 1800, 648, 1244, 1395, 2727, 1876, 3305, 34, 3336, 2982, 1735, 2085, 3179, 2293, 1781, 2452, 3008, 2267, 1226, 998, 3130, 2680, 1389, 1549, 504, 298, 1729, 2615, 3340, 1870, 1385, 427, 2241, 1365, 1204, 1868, 2193, 2970, 2871, 2440, 1739, 2339, 2000, 1479, 119, 203, 952, 2719, 2197, 629, 1994, 951, 2492, 1806, 1256, 2093, 261, 1081, 1341, 3380, 1455, 3193, 2966, 1334, 2379, 677, 814, 240, 1410, 2510, 194, 254, 2102, 3208, 3093, 2736, 845, 3377, 1564, 2396, 2562, 2142, 562, 2732, 755, 375, 1200, 689, 274, 3379, 1161, 2394, 1778, 3290, 302, 2614, 2494, 1417, 2065, 2567, 593, 813, 312, 405, 943, 734, 1972, 2910, 2409, 1440, 1486, 2188, 3137, 1379, 3119, 2715, 3160, 818, 1251, 971, 199, 1598, 3110, 1583, 3054, 3244, 1648, 3155, 778, 1106, 2922, 2463, 1235, 987, 2219, 510, 503, 1567, 1713, 489, 223, 3036, 2202, 625, 567, 3124, 961, 2701, 1808, 1650, 2432, 2557, 327, 252, 77, 1637, 2907, 1333, 1949, 2906, 1487, 87, 3302, 1496, 3157, 1988, 3373, 3219, 3002, 653, 104, 155, 2216, 1720, 1841, 1084, 637, 355, 3294, 2983, 1751, 714, 1692, 1447, 2033, 2873, 1906, 922, 807, 1366, 1782, 2242, 3067, 2205, 416, 2619, 2971, 789, 680, 174, 68, 2116, 1823, 60, 1258, 870, 2962, 246, 2015, 170, 3052, 348, 2788, 98, 1398, 2552, 2303, 251, 1407, 940, 2540, 1281, 2883, 2011, 979, 924, 2288, 1632, 2747, 1671, 2992, 1828, 3346, 795, 775, 447, 2664, 1900, 2981, 1759, 2728, 1899, 760, 1327, 608, 613, 217, 937, 3097, 3014, 2261, 2581, 17, 1766, 111, 913, 305, 2897, 1547, 1216, 2333, 985, 1441, 141, 1323, 32, 143, 1700, 541, 2098, 2627, 2174, 1276, 840, 2039, 2298, 1476, 2466, 2170, 3365, 31, 3189, 2386, 1769, 3329, 1291, 202, 2604, 3282, 2370, 2458, 2699, 708, 2164, 2964, 1662, 1869, 2328, 1653, 1137, 1209, 2525, 2273, 2226, 2623, 738, 2780, 1805, 322, 57, 278, 2811, 1831, 2051, 1286, 2408, 299, 607, 2645, 103, 3375, 2575, 2943, 2161, 2278, 2004, 2377, 3186, 1981, 2420, 2469, 977, 1151, 2374, 1109, 1308, 536, 1790, 2703, 693, 2506, 1135, 160, 55, 2058, 2073, 1265, 3366, 2698, 939, 2390, 914, 101, 20, 1942, 645, 891, 3051, 288, 2012, 935, 851, 2822, 1171, 1990, 1989, 2521, 2165, 1596, 2453, 122, 2434, 2268, 1570, 180, 1693, 1902, 1386, 2173, 2198, 1627, 1908, 2937, 506, 3, 1576, 3029, 863, 1040, 792, 954, 2729, 1364, 962, 309, 1984, 2059, 1148, 2995, 1099, 1765, 402, 173, 58, 3209, 2963, 1741, 3040, 2325, 2598, 928, 2251, 2843, 3268, 169, 2100, 2837, 1837, 2785, 3059, 2192, 2300, 3165, 2431, 2042, 1358, 2676, 2290, 2467, 1861, 2626, 716, 383, 1085, 2848, 413, 3145, 1374, 2097, 1445, 1618, 2813, 1911, 3116, 3281, 3222, 2233, 1457, 1302, 2306, 723, 1797, 2182, 2157, 2052, 1951, 433, 1092, 2447, 1023, 1141, 1610, 712, 3162, 1879, 2545, 377, 1193, 1854, 3321, 1580, 917, 1843, 2835, 177, 2893, 313, 1810, 1408, 2800, 1631, 2375, 2383, 196, 2476, 1591, 2415, 815, 1046, 353, 2292, 1999, 1982, 843, 2695, 1420, 3312, 142, 969, 1626, 3001, 2047, 9, 301, 1714, 1303, 76, 1059, 3369, 1012, 1354, 1466, 16, 768, 1675, 1777, 551, 616, 3066, 1280, 1115, 1634, 490, 226, 3152, 1651, 781, 2869, 2348, 1516, 1915, 3147, 806, 824, 672, 2734, 650, 465, 2709, 1246, 1351, 1315, 563, 2577, 2477, 1485, 1850, 2854, 404, 191, 1359, 1985, 1604, 895, 1353, 339, 1477, 990, 2247, 910, 2380, 2754, 878, 2642, 1644, 2726, 1004, 319, 2478, 1483, 2095, 731, 732, 1538, 2482, 237, 429, 70, 2658, 1144, 3009, 468, 685, 124, 3360, 748, 15, 453, 1853, 1360, 2607, 1822, 2725, 1660, 1701, 726, 2555, 1482, 2953, 3361, 2232, 3172, 1882, 956, 2904, 2818, 2921, 3074, 2417, 735, 1130, 1295, 1521, 1396, 159, 609, 3259, 3131, 99, 1563, 1862, 2044, 221, 512, 3306, 2815, 2851, 1673, 871, 2384, 2148, 1068, 3181, 575, 3210, 1625, 1372, 162, 2700, 739, 3245, 2327, 2104, 207, 615, 1475, 1948, 3250, 957, 3236, 42, 2947, 3083, 2829, 2403, 2571, 1063, 2693, 675, 2516, 460, 487, 1749, 2599, 670, 688, 3275, 2, 2132, 2451, 2611, 134, 1072]\n"
     ]
    }
   ],
   "source": [
    "TEST_PERCENT = 0.25\n",
    "\n",
    "# Load pgn paths\n",
    "pgns = import_data(5)\n",
    "\n",
    "# Convert pgns to tensors\n",
    "board_tensors, next_moves = parse_pgn_to_tensors(pgns)\n",
    "\n",
    "# Converting the dataset into a custom pytorch one\n",
    "dataset = ChessDataset(board_tensors, next_moves)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Splitting the data into train and test\n",
    "train_dataset, test_data = torch.utils.data.random_split(dataset, [1-TEST_PERCENT, TEST_PERCENT])\n",
    "\n",
    "print(len(test_data))  # Number of states\n",
    "print(train_dataset.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEURAL NETWORK DESIGN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Whether to do the operations on the cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PieceToMoveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Takes as input a tensor of 14 channels (8x8 board)\n",
    "        self.conv1 = nn.Conv2d(14, 6, 3)  # 6 filters, 3x3 kernel\n",
    "        self.pool = nn.MaxPool2d(2, 2)    # Max pooling with 2x2 window\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)  # 16 filters, 3x3 kernel\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(120)\n",
    "        \n",
    "        # If starting with 8x8, after two pool layers it becomes 2x2.\n",
    "        # Output from conv2 will be (16 channels, 1x1 feature maps), flattened to 16 * 1 * 1 = 16\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # Predicts the tile to move the piece from (64 possible tiles on the board)\n",
    "        self.fc3 = nn.Linear(84, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Apply first conv + pooling\n",
    "        x = F.relu(self.conv2(x)) # Apply second conv to get 16, 1, 1\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch size\n",
    "        x = F.relu(self.bn1(self.fc1(x)))  # Fully connected layer 1\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))  # Fully connected layer 2\n",
    "        x = self.fc3(x)          # Output layer (no activation, logits for classification)\n",
    "        return x\n",
    "\n",
    "piece_to_move_net = PieceToMoveNet()\n",
    "# Move the network to gpu / cpu befor initializing the optimizer\n",
    "piece_to_move_net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(piece_to_move_net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING LOOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1752, 724, 356, 2155, 2398, 630, 1726, 1768, 1510, 1001, 1351, 1778, 928, 1844, 2139, 2161, 595, 2498, 2517, 1209, 1568, 582, 213, 1155, 670, 2229, 1365, 789, 1919, 1287, 21, 1037, 554, 368, 1058, 122, 1253, 703, 2401, 515, 1228, 466, 797, 1930, 1457, 1026, 1326, 836, 338, 345, 842, 1206, 43, 221, 2493, 742, 2167, 2131, 652, 1637, 1127, 2156, 146, 1415, 1693, 890, 2427, 2524, 2469, 1409, 26, 2278, 1667, 1475, 438, 1073, 2365, 1136, 2092, 2455, 137, 1937, 2504, 1586, 1269, 611, 2376, 2458, 168, 17, 1103, 2077, 1809, 1248, 1295, 599, 2520, 134, 964, 1898, 869, 1642, 2470, 1271, 1182, 1417, 942, 2227, 1526, 1852, 1602, 756, 1282, 944, 1309, 1051, 1571, 707, 1226, 1679, 1370, 416, 1756, 1246, 1527, 1666, 2509, 151, 651, 2134, 2441, 2461, 455, 994, 1575, 1833, 418, 1604, 1765, 1822, 383, 2110, 564, 184, 2095, 2348, 1823, 1961, 761, 1284, 1217, 1862, 1364, 1874, 193, 1313, 1470, 2253, 1503, 1521, 494, 1781, 1016, 522, 931, 1489, 1926, 427, 2269, 1353, 2232, 1000, 1694, 781, 1339, 2067, 1868, 1100, 1427, 389, 524, 1645, 108, 1135, 1539, 767, 1643, 676, 211, 423, 315, 2425, 261, 2480, 889, 1627, 441, 2026, 1817, 417, 228, 2015, 1425, 865, 2191, 2490, 14, 233, 1237, 214, 292, 817, 1474, 2212, 1464, 204, 71, 878, 288, 1011, 1306, 2084, 2223, 1044, 1199, 216, 1738, 757, 2121, 575, 2260, 1063, 1142, 1400, 1583, 1680, 1590, 2023, 1818, 1724, 673, 1773, 1235, 2321, 829, 2357, 414, 2200, 823, 1962, 1017, 2292, 2193, 1191, 1952, 1932, 2263, 733, 581, 1538, 845, 92, 1605, 1451, 1683, 1129, 681, 442, 1494, 1700, 1894, 634, 1038, 1755, 450, 378, 939, 1291, 1225, 830, 1665, 1152, 1030, 642, 405, 2423, 1414, 1587, 1964, 1708, 255, 2087, 1070, 786, 2185, 1648, 897, 2217, 1515, 546, 936, 1138, 82, 2287, 908, 344, 30, 2244, 2483, 1040, 248, 1900, 2313, 2436, 175, 1008, 1564, 1394, 1549, 459, 702, 1956, 2137, 2479, 1593, 208, 2117, 2446, 590, 1404, 1445, 2283, 999, 2152, 527, 1186, 1762, 1224, 1908, 1532, 949, 1547, 550, 2371, 2288, 475, 502, 1160, 2495, 2385, 1096, 1971, 1202, 1890, 2299, 2456, 256, 1969, 2202, 553, 922, 287, 621, 2157, 1933, 903, 1481, 2293, 1633, 2055, 1739, 1684, 1512, 957, 1187, 1884, 1087, 1660, 2264, 1205, 45, 956, 491, 1397, 822, 685, 1288, 821, 1661, 99, 432, 119, 1356, 1857, 2218, 597, 1086, 1763, 1621, 2468, 2418, 1279, 487, 1234, 706, 1814, 1815, 86, 401, 622, 322, 1139, 2201, 841, 360, 1735, 310, 567, 1, 1897, 877, 1640, 126, 1173, 762, 2432, 1810, 1938, 2500, 1789, 2127, 148, 1389, 1545, 955, 267, 496, 730, 2096, 585, 644, 2350, 2047, 1123, 1996, 2404, 298, 2222, 57, 744, 2025, 1977, 798, 203, 800, 540, 2389, 348, 362, 617, 2307, 1019, 330, 1784, 1323, 2116, 1190, 1499, 2013, 446, 959, 1437, 1420, 1567, 735, 849, 80, 2322, 920, 1140, 1902, 336, 2347, 1242, 2526, 1958, 2368, 1592, 2380, 25, 1393, 2245, 2118, 65, 1872, 2285, 2190, 10, 2043, 2099, 1837, 2230, 128, 1934, 740, 1083, 1760, 1748, 731, 526, 2135, 749, 1993, 1305, 485, 2017, 2450, 1378, 1766, 156, 309, 2093, 1991, 1879, 2103, 264, 369, 696, 2180, 1513, 1130, 1032, 1877, 2048, 373, 1467, 379, 1413, 2515, 396, 628, 406, 657, 253, 1657, 276, 1654, 589, 1319, 1366, 392, 818, 2433, 538, 658, 1099, 1064, 1677, 2080, 2011, 556, 2220, 1056, 77, 528, 2381, 1573, 2332, 2340, 1023, 1691, 349, 2219, 1970, 785, 1153, 328, 445, 1616, 317, 2046, 1751, 2209, 29, 637, 2523, 1150, 940, 2166, 713, 2138, 7, 444, 1638, 697, 941, 372, 76, 381, 291, 1090, 1753, 1675, 2508, 1989, 1921, 1398, 89, 2486, 1402, 763, 2140, 90, 2369, 1595, 848, 500, 2106, 2178, 774, 1682, 819, 610, 1698, 1620, 470, 145, 963, 239, 850, 1021, 1703, 2109, 2286, 402, 408, 974, 1686, 107, 1407, 1452, 1935, 1345, 1697, 1863, 316, 2434, 2467, 243, 1301, 934, 1795, 2408, 2112, 481, 1025, 165, 1165, 1461, 1613, 1273, 910, 1820, 347, 2189, 1957, 1033, 2123, 1607, 472, 1331, 2107, 235, 2535, 258, 650, 1794, 675, 2428, 1247, 1054, 1829, 2054, 2083, 1695, 1231, 101, 2243, 791, 831, 1978, 537, 820, 2258, 1570, 1472, 666, 303, 2105, 2104, 2261, 700, 544, 852, 186, 1903, 340, 932, 1622, 2442, 2172, 516, 1004, 2471, 1536, 410, 1007, 1656, 2076, 2396, 1721, 576, 1022, 2362, 2388, 1501, 534, 574, 2276, 8, 313, 555, 1448, 2079, 1965, 1895, 95, 1625, 1075, 1790, 1701, 2266, 1102, 2305, 2482, 39, 981, 1787, 1636, 1758, 318, 1600, 1441, 1732, 2477, 183, 1216, 1922, 716, 1786, 1239, 837, 56, 2257, 1864, 150, 1212, 3, 729, 13, 1286, 448, 1606, 1910, 1634, 2064, 1266, 376, 807, 461, 1307, 1151, 872, 66, 2164, 1582, 157, 1117, 1548, 2196, 1045, 1189, 2057, 2354, 1670, 2444, 1869, 1764, 1042, 249, 868, 191, 631, 1428, 1896, 2126, 710, 712, 2525, 2415, 158, 2533, 1195, 806, 714, 212, 1357, 1944, 1792, 343, 1791, 2290, 1659, 1168, 990, 975, 2113, 862, 1537, 334, 498, 2355, 2122, 766, 1133, 660, 2464, 989, 1994, 2097, 2416, 880, 2334, 765, 884, 810, 1976, 387, 155, 1873, 1678, 2205, 1068, 62, 1947, 2391, 2302, 419, 353, 1967, 2007, 1092, 1263, 1002, 665, 161, 265, 2114, 1826, 1430, 1316, 6, 411, 991, 755, 854, 1893, 97, 876, 187, 2314, 1983, 2153, 1433, 571, 1514, 164, 59, 2353, 2315, 2254, 1077, 2352, 993, 479, 449, 179, 74, 1779, 926, 2240, 1649, 1013, 686, 1434, 2351, 695, 246, 2280, 911, 33, 2268, 1848, 2027, 2021, 314, 1028, 1471, 1821, 1672, 493, 1835, 594, 36, 2492, 2041, 2298, 428, 1101, 1478, 1966, 782, 1422, 2001, 1399, 1419, 893, 1737, 2310, 2360, 458, 2238, 346, 2002, 531, 1124, 885, 2277, 1327, 805, 218, 112, 1085, 961, 2040, 514, 2326, 682, 1375, 1329, 856, 2466, 689, 587, 1999, 1296, 1831, 1772, 1486, 1974, 1012, 867, 768, 275, 647, 266, 952, 1734, 1418, 1377, 953, 1047, 653, 23, 1082, 1031, 1338, 2035, 1614, 1061, 2228, 1574, 321, 2061, 1121, 215, 1249, 277, 1293, 1855, 2294, 201, 93, 1280, 1039, 1733, 813, 2414, 244, 1651, 1878, 1369, 1828, 1076, 1006, 573, 2460, 751, 2063, 2056, 600, 2216, 1841, 542, 772, 2519, 1423, 78, 271, 1178, 132, 2413, 1392, 1509, 468, 2, 1886, 1997, 198, 638, 1754, 1196, 627, 38, 1317, 892, 2183, 154, 2239, 329, 1610, 87, 2319, 1439, 1466, 1463, 2029, 1106, 748, 84, 53, 844, 2297, 1520, 12, 1866, 1015, 1771, 1584, 1088, 2349, 2316, 833, 2405, 2431, 2312, 1853, 2199, 619, 2160, 709, 1416, 1881, 489, 456, 916, 2327, 327, 367, 1557, 693, 1320, 669, 1628, 139, 1832, 769, 15, 954, 73, 337, 206, 1995, 1125, 655, 1349, 983, 561, 364, 1334, 2356, 2378, 2393, 1175, 1949, 488, 1715, 1368, 792, 117, 1681, 403, 1241, 94, 2320, 996, 728, 2146, 1277, 1729, 304, 98, 2102, 170, 2170, 1438, 2130, 794, 511, 1525, 602, 948, 1294, 620, 616, 946, 1816, 883, 1807, 1412, 27, 1631, 2141, 783, 1856, 465, 1838, 832, 1747, 16, 796, 1788, 1108, 1543, 1220, 726, 1222, 577, 1901, 584, 2247, 1845, 2491, 2417, 1709, 1444, 1299, 1782, 2221, 1920, 2438, 1652, 900, 41, 2308, 1156, 824, 2435, 1066, 1518, 240, 1122, 1940, 1029, 1105, 2306, 1259, 603, 1759, 2181, 839, 1097, 1533, 1802, 2052, 323, 2284, 1113, 687, 2358, 462, 1836, 471, 225, 971, 490, 1646, 2120, 247, 2309, 2337, 1344, 705, 799, 998, 606, 238, 1302, 2125, 1800, 1114, 361, 1931, 1046, 1260, 598, 1719, 1722, 2236, 1776, 1109, 115, 1770, 1981, 2339, 226, 1488, 259, 882, 159, 257, 250, 826, 2453, 1972, 664, 1084, 1516, 1644, 1824, 110, 377, 1111, 572, 671, 400, 1783, 912, 114, 260, 1074, 1119, 1390, 1107, 2367, 834, 1707, 1671, 2510, 679, 629, 1089, 2330, 770, 2474, 440, 758, 913, 2363, 943, 863, 1065, 2065, 1261, 811, 1591, 661, 335, 2507, 1252, 2179, 1110, 523, 393, 1204, 46, 545, 2346, 453, 352, 950, 1806, 1473, 2390, 1485, 1870, 2419, 113, 2145, 296, 2213, 268, 2499, 1871, 2008, 2522, 1324, 2000, 2171, 2088, 1079, 891, 640, 1906, 2303, 1468, 142, 486, 1262, 2527, 2175, 1146, 290, 1245, 708, 2094, 1850, 2451, 1379, 1361, 2386, 698, 2475, 375, 476, 1492, 1450, 2279, 741, 136, 1632, 874, 1382, 365, 63, 420, 1132, 2066, 431, 1840, 2252, 2516, 1447, 1812, 1825, 2129, 937, 1761, 1429, 162, 49, 2241, 1544, 1577, 501, 1244, 477, 2010, 370, 380, 759, 1221, 2440, 1805, 1950, 169, 1696, 222, 452, 2409, 104, 1386, 668, 773, 533, 1502, 91, 2225, 1347, 426, 718, 1340, 2207, 1552, 543, 1207, 2059, 1531, 1371, 1540, 1069, 2014, 1292, 1827, 1250, 1909, 388, 1612, 1055, 2119, 1865, 795, 2462, 308, 79, 263, 31, 51, 2081, 2503, 1887, 683, 2472, 2177, 223, 232, 678, 1565, 563, 1383, 483, 1227, 927, 1372, 1562, 1052, 779, 1980, 802, 32, 843, 2038, 19, 873, 1959, 1480, 1333, 1851, 34, 915, 1067, 2437, 202, 980, 2532, 2273, 407, 2489, 325, 2176, 2397, 1174, 2124, 2341, 1550, 391, 2512, 945, 1240, 302, 586, 840, 324, 513, 1373, 11, 787, 933, 721, 102, 1804, 804, 2037, 1954, 2022, 851, 326, 1405, 855, 717, 1321, 1161, 618, 385, 1219, 771, 732, 694, 1720, 656, 2203, 816, 1505, 188, 684, 2195, 505, 1354, 273, 1635, 753, 1104, 529, 1408, 1608, 163, 1455, 395, 960, 521, 1381, 632, 1617, 692, 1421, 1917, 2361, 958, 530, 284, 85, 720, 1603, 422, 2318, 2073, 1352, 58, 1846, 1718, 1387, 384, 354, 1953, 1403, 2429, 300, 633, 1048, 1963, 1050, 2198, 1563, 2082, 20, 1630, 430, 1572, 190, 914, 443, 935, 1143, 1524, 2133, 173, 269, 1270, 1322, 182, 2422, 1275, 319, 171, 1988, 2091, 2410, 1924, 1799, 144, 1774, 1655, 123, 138, 1624, 189, 1491, 48, 643, 985, 967, 2530, 508, 2270, 1148, 2387, 194, 1830, 1580, 1164, 1745, 1736, 2373, 2407, 977, 1542, 1385, 24, 1883, 464, 2111, 217, 2246, 1984, 2267, 739, 760, 614, 1757, 559, 2473, 1588, 1328, 305, 484, 42, 938, 1459, 44, 413, 237, 2142, 645, 2132, 1255, 1834, 558, 1508, 1973, 1094, 667, 607, 270, 2018, 1391, 2100, 517, 1767, 2168, 1465, 192, 1793, 734, 688, 2333, 860, 2194, 966, 1210, 2511, 1912, 1348, 562, 1490, 1230, 896, 2531, 2295, 127, 1278, 2289, 109, 988, 0, 2518, 35, 219, 2342, 1669, 1053, 1702, 1342, 899, 339, 1020, 2248, 2262, 552, 1367, 1618, 887, 548, 2030, 2487, 2174, 1497, 1936, 968, 230, 429, 2158, 121, 2039, 2034, 722, 332, 1819, 1453, 1534, 1731, 1581, 2231, 809, 970, 1522, 987, 1899, 1674, 1685, 525, 1928, 1867, 297, 764, 2291, 601, 1095, 436, 750, 1194, 1911, 2374, 1057, 1713, 2336, 1443, 1009, 2045, 2449, 2463, 2421, 2411, 424, 2184, 1601, 135, 1162, 864, 565, 947, 1192, 1154, 1341, 1711, 1673, 199, 245, 180, 1506, 2226, 1460, 167, 704, 853, 47, 1454, 1541, 1943, 719, 2412, 1529, 2275, 1469, 2003, 2215, 1308, 1647, 1098, 2250, 1170, 1406, 2323, 281, 1314, 1131, 2208, 409, 924, 519, 231, 2452, 1705, 1663, 124, 166, 2394, 294, 1350, 2012, 881, 153, 312, 506, 1915, 1948, 9, 397, 835, 386, 507, 435, 2485, 61, 747, 54, 551, 2136, 1435, 72, 2031, 1854, 646, 1558, 1355, 2325, 871, 1312, 282, 149, 252, 398, 2188, 473, 133, 1990, 752, 1315, 1916, 1609, 2069, 1615, 371, 1528, 588, 1594, 1746, 1211, 2234, 2211, 1585, 1714, 1493, 1560, 1360, 1927, 1172, 1116, 1254, 1530, 962, 447, 480, 2192, 1556, 1376, 1401, 106, 1885, 1141, 2224, 592, 1363, 2476, 196, 1188, 1449, 1300, 1337, 2182, 861, 1623, 2448, 1727]\n",
      "EPOCH 1: \n",
      "LOSS train 1.3748766998449962, valid 1.6908218681812286\n",
      "EPOCH 2: \n",
      "LOSS train 1.365602844953537, valid 1.6888993203639984\n",
      "EPOCH 3: \n",
      "LOSS train 1.3566044708093008, valid 1.6852058172225952\n",
      "EPOCH 4: \n",
      "LOSS train 1.3611169755458832, valid 1.6931067168712617\n",
      "EPOCH 5: \n",
      "LOSS train 1.346223384141922, valid 1.683175539970398\n",
      "EPOCH 6: \n",
      "LOSS train 1.3564651469389597, valid 1.6925209283828735\n",
      "EPOCH 7: \n",
      "LOSS train 1.3494676768779754, valid 1.69277104139328\n",
      "EPOCH 8: \n",
      "LOSS train 1.3452195306619008, valid 1.7085469245910645\n",
      "EPOCH 9: \n",
      "LOSS train 1.3352398733297983, valid 1.7168702483177185\n",
      "EPOCH 10: \n",
      "LOSS train 1.33846315741539, valid 1.7167148053646089\n",
      "EPOCH 11: \n",
      "LOSS train 1.3301375011603038, valid 1.7069320857524872\n",
      "EPOCH 12: \n",
      "LOSS train 1.3286692480246225, valid 1.7090867161750793\n",
      "EPOCH 13: \n",
      "LOSS train 1.318124876419703, valid 1.7076986730098724\n",
      "EPOCH 14: \n",
      "LOSS train 1.302242464820544, valid 1.7114435732364655\n",
      "EPOCH 15: \n",
      "LOSS train 1.329424907763799, valid 1.7270439207553863\n",
      "EPOCH 16: \n",
      "LOSS train 1.3170266807079316, valid 1.7281618177890778\n",
      "EPOCH 17: \n",
      "LOSS train 1.3086273183425268, valid 1.7237268567085267\n",
      "EPOCH 18: \n",
      "LOSS train 1.3021505316098532, valid 1.7382788836956025\n",
      "EPOCH 19: \n",
      "LOSS train 1.296444833278656, valid 1.7251552402973176\n",
      "EPOCH 20: \n",
      "LOSS train 1.3045257131258647, valid 1.7257526576519013\n",
      "EPOCH 21: \n",
      "LOSS train 1.2997691025336584, valid 1.7356930792331695\n",
      "EPOCH 22: \n",
      "LOSS train 1.2879976650079092, valid 1.741225916147232\n",
      "EPOCH 23: \n",
      "LOSS train 1.2697292000055314, valid 1.73634232878685\n",
      "EPOCH 24: \n",
      "LOSS train 1.2795836766560873, valid 1.745108139514923\n",
      "EPOCH 25: \n",
      "LOSS train 1.2727193961540857, valid 1.7500219285488128\n",
      "EPOCH 26: \n",
      "LOSS train 1.2677126079797745, valid 1.7456156492233277\n",
      "EPOCH 27: \n",
      "LOSS train 1.2751086781422296, valid 1.7628322780132293\n",
      "EPOCH 28: \n",
      "LOSS train 1.279457300901413, valid 1.750201165676117\n",
      "EPOCH 29: \n",
      "LOSS train 1.2751940488815308, valid 1.7581379771232606\n",
      "EPOCH 30: \n",
      "LOSS train 1.2516308814287185, valid 1.7535155296325684\n",
      "EPOCH 31: \n",
      "LOSS train 1.2655366281668345, valid 1.7580393195152282\n",
      "EPOCH 32: \n",
      "LOSS train 1.2649536748727164, valid 1.7533838868141174\n",
      "EPOCH 33: \n",
      "LOSS train 1.2734474509954452, valid 1.763220477104187\n",
      "EPOCH 34: \n",
      "LOSS train 1.2685583919286727, valid 1.7572841465473175\n",
      "EPOCH 35: \n",
      "LOSS train 1.241201318303744, valid 1.7741376996040343\n",
      "EPOCH 36: \n",
      "LOSS train 1.2454410433769225, valid 1.776253455877304\n",
      "EPOCH 37: \n",
      "LOSS train 1.250346478819847, valid 1.7805006980895997\n",
      "EPOCH 38: \n",
      "LOSS train 1.2346022655566533, valid 1.782862800359726\n",
      "EPOCH 39: \n",
      "LOSS train 1.2390856782595316, valid 1.7952860057353974\n",
      "EPOCH 40: \n",
      "LOSS train 1.2409979939460754, valid 1.7806867003440856\n",
      "EPOCH 41: \n",
      "LOSS train 1.2330837349096935, valid 1.7918945014476777\n",
      "EPOCH 42: \n",
      "LOSS train 1.2397909760475159, valid 1.786896014213562\n",
      "EPOCH 43: \n",
      "LOSS train 1.2192241529623666, valid 1.7877676367759705\n",
      "EPOCH 44: \n",
      "LOSS train 1.2169512867927552, valid 1.80480996966362\n",
      "EPOCH 45: \n",
      "LOSS train 1.204503779609998, valid 1.7918313145637512\n",
      "EPOCH 46: \n",
      "LOSS train 1.213213082154592, valid 1.787673246860504\n",
      "EPOCH 47: \n",
      "LOSS train 1.2087182462215424, valid 1.8029781877994537\n",
      "EPOCH 48: \n",
      "LOSS train 1.2029664059480032, valid 1.7969411730766296\n",
      "EPOCH 49: \n",
      "LOSS train 1.2085353553295135, valid 1.8103983938694\n",
      "EPOCH 50: \n",
      "LOSS train 1.2093993445237479, valid 1.8045818567276002\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(f\"runs/piece_to_move_{timestamp}\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_data, validation_data = torch.utils.data.random_split(train_dataset, [1-TEST_PERCENT, TEST_PERCENT])\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(train_data, BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "def train_one_epoch(epoch_index: int, tb_writer, optimizer, training_loader, loss_fn): \n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs = data[0]\n",
    "        # Extracting only the tile of the piece to move\n",
    "        labels = data[1][:, 0]\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = piece_to_move_net(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999: \n",
    "            last_loss = running_loss / 1000\n",
    "            print(f\" batch {i + 1}, loss: {last_loss}\")\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar(\"Loss/train\", last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    \n",
    "    if last_loss == 0.:\n",
    "        last_loss = running_loss / (i+1)\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def train_multiple_epochs(n_epochs, model, writer, validation_loader, loss_fn):\n",
    "    best_vloss = 1_000_000\n",
    "\n",
    "    for epoch in range(EPOCHS): \n",
    "        print(f\"EPOCH {epoch + 1}: \")\n",
    "\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch, writer, optimizer, training_loader, loss_fn)\n",
    "        running_vloss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_vbatches = 0\n",
    "        with torch.no_grad():\n",
    "            for i, v_data in enumerate(validation_loader):\n",
    "                \n",
    "                vinputs = v_data[0]\n",
    "                vlabels = v_data[1][:,0]\n",
    "\n",
    "                vinputs = vinputs.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "        avg_vloss = running_vloss / (i+1)\n",
    "        print(f\"LOSS train {avg_loss}, valid {avg_vloss}\")\n",
    "\n",
    "        writer.add_scalars(\"Training vs Validation Loss\", {\n",
    "            \"Training\": avg_loss, \"Validation\": avg_vloss}, \n",
    "            epoch + 1)\n",
    "        writer.flush()\n",
    "        \n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_loss\n",
    "            model_path = f\"model_{timestamp}_{epoch}\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "\n",
    "train_multiple_epochs(EPOCHS, piece_to_move_net, writer, validation_loader, loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(board: chess.Board, outputs: np.arry, labels: np.array) -> np.array:\n",
    "    \"\"\"Creates mask with legal moves from the current board state\"\"\"\n",
    "\n",
    "    mask = np.zeros((8,8)) # 8x8 mask for the chessboard\n",
    "\n",
    "    # Obtaining legal moves from the board\n",
    "    legal_moves = list(board.legal_moves)\n",
    "\n",
    "    # Indicating with 1s the valid squares\n",
    "    for move in legal_moves:\n",
    "        to_square = move.to_square\n",
    "        to_row, to_col = divmod(to_square, 8)\n",
    "        mask[to_row, to_col] = 1 # A valid square\n",
    "\n",
    "    # Reshaping mask to match output and labels\n",
    "    move_mask = mask.flatten() # Converts 8*8 2D array to a 1D array with 64 elements\n",
    "\n",
    "    masked_outputs = outputs * move_mask\n",
    "    masked_labels = labels * move_mask\n",
    "\n",
    "    return masked_outputs, masked_labels\n",
    "\n",
    "def update_board( board: chess.Board, move: chess.Move ) -> chess.Board:\n",
    "    \"\"\"This function is responsible for updating the board everytime a move is made\"\"\"\n",
    "\n",
    "    board.push(move) # Add move to the board\n",
    "    \n",
    "    return move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CROSS VALIDATION**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    \"\"\"Resets the weights of the model, so the model is trained with randomly initalized weights\"\"\"\n",
    "\n",
    "    # List of layers containing reset parameters\n",
    "    layer_types = [nn.Conv2d, nn.Linear, nn.BatchNorm2d]\n",
    "\n",
    "    # Iterating through all layers of the model\n",
    "    for layer in model.modules():\n",
    "        # Check layers with reset parameters\n",
    "        if type(layer) in layer_types:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
