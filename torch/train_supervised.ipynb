{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aux_functions\n",
    "import importlib\n",
    "\n",
    "importlib.reload(aux_functions)\n",
    "from aux_functions import *\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.masked import masked_tensor\n",
    "\n",
    "import numpy as np\n",
    "import chess\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PROCESSING**\n",
    "\n",
    "- Importing the pgn data\n",
    "- Transforming the data to sparce tensors \n",
    "- Splitting the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2537\n",
      "[414, 235, 1355, 1009, 437, 1064, 2105, 260, 2175, 81]\n"
     ]
    }
   ],
   "source": [
    "TEST_PERCENT = 0.25\n",
    "\n",
    "# Load pgn paths\n",
    "pgns = import_data(5)\n",
    "\n",
    "# Convert pgns to tensors\n",
    "board_tensors, next_moves = parse_pgn_to_tensors(pgns)\n",
    "\n",
    "# Converting the dataset into a custom pytorch one\n",
    "dataset = ChessDataset(board_tensors, next_moves)\n",
    "\n",
    "# Setting manual seed so that the split always has the same indexes \n",
    "torch.manual_seed(0)\n",
    "# Splitting the data into train and test\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [1-TEST_PERCENT, TEST_PERCENT])\n",
    "\n",
    "print(len(train_dataset))  \n",
    "print(train_dataset.indices[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEURAL NETWORK DESIGN**\n",
    "- 2 Convolutional layers\n",
    "- 2 Fully connected hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to do the operations on the cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Neural network to predict which piece to move\n",
    "class PieceToMoveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Takes as input a tensor of 14 channels (8x8 board)\n",
    "        self.conv1 = nn.Conv2d(14, 6, 3)  # 6 filters, 3x3 kernel\n",
    "        self.pool = nn.MaxPool2d(2, 2)    # Max pooling with 2x2 window\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)  # 16 filters, 3x3 kernel\n",
    "\n",
    "        # Using droput to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Using batch normalization to make training faster and more stable\n",
    "        self.bn1 = nn.BatchNorm1d(120)  # For the 1st layer\n",
    "        self.bn2 = nn.BatchNorm1d(84)   # For the 2nd layer\n",
    "        \n",
    "        # Output from conv2 will be (16 channels, 1x1 feature maps)\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)\n",
    "        # First hidden layer with 120 inputs and 84 outputs\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # Second hidden layer with 84 inputs and 64 outputs (board tiles)\n",
    "        self.fc3 = nn.Linear(84, 64)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional layer and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x))) \n",
    "        # Second convolutional layer (no pooling needed)\n",
    "        x = F.relu(self.conv2(x)) \n",
    "        # Flatten all dimensions except batch size            \n",
    "        x = torch.flatten(x, 1)       \n",
    "\n",
    "        # Fully connected layer 1 and batch normalization\n",
    "        x = F.relu(self.bn1(self.fc1(x)))    \n",
    "        # Dropout neurons from the first layer to reduce overfitting\n",
    "        x = self.dropout(x)    \n",
    "        # Fully connected layer 2 and batch normalization               \n",
    "        x = F.relu(self.bn2(self.fc2(x)))  \n",
    "        # Output layer (no activation, logits for classification)   \n",
    "        x = self.fc3(x)         \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING LOOP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a mask on which pieces the NN can move based on the current board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(tensor) -> list:\n",
    "    \"\"\"Generates a mask which contains the position of the pieces that can be moved\"\"\"\n",
    "\n",
    "    # If layer 12 has any 1 it will be whites turn\n",
    "    if  torch.any(tensor[12] == 1):\n",
    "        # White pieces are in layers 0 to 5, summing across those layers\n",
    "        mask = torch.sum(tensor[0:6], dim = 0).int()\n",
    "\n",
    "\n",
    "    # If layer 13 has any 1 it will be blacks turn\n",
    "    elif torch.any(tensor[13] == 1):\n",
    "        # Black pieces are in layers 6 to 11, summing across those layers\n",
    "        mask = torch.sum(tensor[6:12], dim = 0).int()\n",
    "\n",
    "    # Returning the masking in a list format\n",
    "    return mask.flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training the model on one epoch\n",
    "- Validating the model on one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current time\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, loss_fn, train_sampler_size): \n",
    "    \"\"\"Trains the model for one epoch and returns the average training loss and accuracy\"\"\"\n",
    "\n",
    "    # Initializing the avg. loss and correct guesses\n",
    "    running_loss = 0.  \n",
    "    running_correct = 0.\n",
    "\n",
    "    # Looping through all samples in a batch\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # Extracting the board tensor\n",
    "        inputs = data[0]\n",
    "        # Extracting the tile of the piece to move\n",
    "        labels = data[1]\n",
    "\n",
    "        # Resetting the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculating the mask for the current position\n",
    "        mask = [generate_mask(pos) for pos in inputs]\n",
    "        mask = torch.tensor(mask)\n",
    "        \n",
    "        # Moving inputs, labels and mask to the gpu/cpu\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Calculating the masked output\n",
    "        logits = model(inputs)\n",
    "        outputs = logits * mask.float()\n",
    "\n",
    "        # Calculating the sample loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # Calculating the gradient with respect to the loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Updating model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adding the last loss to the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate number of correct predictions\n",
    "        _, predictions = torch.max(outputs.data, 1)\n",
    "        running_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    # Averaging the loss for all samples in the batch\n",
    "    running_loss /= (i + 1)\n",
    "\n",
    "    # Calculate accuracy based on the total samples in the fold (train_sampler_size)\n",
    "    train_accuracy = running_correct / train_sampler_size\n",
    "\n",
    "    return running_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validation_epoch(model, validation_loader, loss_fn, val_sampler_size):\n",
    "    \"\"\"Validates the model for one epoch and returns the average validation loss and accuracy\"\"\"\n",
    "\n",
    "    # Initializes the validation loss and correct guesses\n",
    "    running_vloss = 0.\n",
    "    running_vcorrect = 0.\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations for validation set\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Looping through all batches in the validation set\n",
    "        for i, v_data in enumerate(validation_loader):\n",
    "\n",
    "            # Getting the tensors of the validation data\n",
    "            vinputs = v_data[0]\n",
    "            vlabels = v_data[1] \n",
    "\n",
    "            # Calculating the mask for the current position\n",
    "            mask = [generate_mask(pos) for pos in vinputs]\n",
    "            mask = torch.tensor(mask)\n",
    "\n",
    "            # Moving inputs, labels and mask to the gpu/cpu\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            # Calculating the masked output\n",
    "            logits = model(vinputs)\n",
    "            voutputs = logits * mask.float()\n",
    "\n",
    "            # Calculating the loss of the model in the validation sample\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "\n",
    "            # Adding this sample's loss to the total loss\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "            # Calculate number of correct predictions\n",
    "            _, predictions = torch.max(voutputs.data, 1)\n",
    "            running_vcorrect += (predictions == vlabels).sum().item()\n",
    "\n",
    "    # Averaging the loss for all samples in the validation set\n",
    "    running_vloss /= (i + 1)\n",
    "\n",
    "    # Calculate accuracy based on the total samples in the fold (val_sampler_size)\n",
    "    validation_accuracy = running_vcorrect / val_sampler_size\n",
    "\n",
    "    return running_vloss, validation_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing cross validation while training the model on multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple_folds(n_epochs, n_folds, batch_size, splits, writer, optimizer_class, optimizer_params, loss_fn):\n",
    "    \"\"\"Trains and validates the model on multiple folds\"\"\"\n",
    "\n",
    "    # Initializing best validation loss of the model\n",
    "    best_vloss = 1_000\n",
    "\n",
    "    # Initializing losses and accuracies for training and validation to save them on TensorBoard\n",
    "    epochs_tloss = [0 for _ in range(n_epochs)]\n",
    "    epochs_tacc = [0 for _ in range(n_epochs)]\n",
    "    epochs_vloss = [0 for _ in range(n_epochs)]\n",
    "    epochs_vacc = [0 for _ in range(n_epochs)]\n",
    "    \n",
    "    # Looping through all folds for cross validation \n",
    "    for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "\n",
    "        print(f\"FOLD {fold+1}\")\n",
    "\n",
    "        # Getting the sampler for training and validation\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        # Loaders for training and validation\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        # Resetting the model each fold\n",
    "        model = PieceToMoveNet()\n",
    "        model.to(device)\n",
    "\n",
    "        # Resetting the optimizer and its parameters each fold\n",
    "        optimizer = optimizer_class(model.parameters(), **optimizer_params)\n",
    "        \n",
    "        # Getting the sample sizes of training and validation on this fold\n",
    "        train_sampler_size = len(train_sampler)\n",
    "        val_sampler_size = len(val_sampler)\n",
    "\n",
    "        # Looping through all epochs\n",
    "        for epoch in range(n_epochs): \n",
    "            # Training the model one epoch\n",
    "            train_loss, train_acc = train_epoch(model, optimizer, train_loader, loss_fn, train_sampler_size)\n",
    "            # Validating the model on one epoch\n",
    "            val_loss, val_acc = validation_epoch(model, val_loader, loss_fn, val_sampler_size)\n",
    "\n",
    "            # Adding losses and accuracies for insights \n",
    "            epochs_tloss[epoch] += train_loss\n",
    "            epochs_tacc[epoch] += train_acc\n",
    "            epochs_vloss[epoch] += val_loss\n",
    "            epochs_vacc[epoch] += val_acc\n",
    "\n",
    "            # Printing insights\n",
    "            print(f\"Epoch: {epoch + 1} Train Loss: {train_loss}, Valid Loss: {val_loss} |\\\n",
    "                   Train Acc: {train_acc}, Valid Acc: {val_acc}\")\n",
    "\n",
    "            # Saving the model if the loss on the validation is lower than the best one\n",
    "            if val_loss < best_vloss:\n",
    "                best_vloss = val_loss\n",
    "                model_path = f\"models/piece_to_move_net_{timestamp}_{fold+1}_{epoch+1}\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Averaging losses and accuracies on each fold\n",
    "    for i in range(n_epochs):\n",
    "        epochs_tloss[i] /= (n_folds)\n",
    "        epochs_tacc[i] /= (n_folds)\n",
    "        epochs_vloss[i] /= (n_folds)\n",
    "        epochs_vacc[i] /= (n_folds)\n",
    "\n",
    "    # Saving losses and accuracies on TensorBoard\n",
    "    for i in range(n_epochs):\n",
    "        # Adding insights\n",
    "        writer.add_scalars(\"Loss\", {\"Training\": epochs_tloss[i], \"Validation\": epochs_vloss[i]}, i + 1)\n",
    "        writer.add_scalars(\"Accuracy\", {\"Training\": epochs_tacc[i], \"Validation\": epochs_vacc[i]}, i + 1)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally training and validating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Epoch: 1 Train Loss: 4.138028810609062, Valid Loss: 4.122337129380968 |                   Train Acc: 0.10585452395032525, Valid Acc: 0.11465721040189125\n",
      "Epoch: 2 Train Loss: 4.068300278681629, Valid Loss: 4.010005385787399 |                   Train Acc: 0.1283264340626848, Valid Acc: 0.1347517730496454\n",
      "Epoch: 3 Train Loss: 3.8835944004778593, Valid Loss: 3.7629760371314154 |                   Train Acc: 0.15079834417504434, Valid Acc: 0.13238770685579196\n",
      "Epoch: 4 Train Loss: 3.5303423899524615, Valid Loss: 3.3750836319393582 |                   Train Acc: 0.15789473684210525, Valid Acc: 0.14420803782505912\n",
      "Epoch: 5 Train Loss: 3.1052874169259703, Valid Loss: 2.9821939821596497 |                   Train Acc: 0.1785925487876996, Valid Acc: 0.16784869976359337\n",
      "Epoch: 6 Train Loss: 2.747179643163141, Valid Loss: 2.69347106968915 |                   Train Acc: 0.1856889414547605, Valid Acc: 0.17494089834515367\n",
      "Epoch: 7 Train Loss: 2.521464064436139, Valid Loss: 2.51214771800571 |                   Train Acc: 0.19574216439976344, Valid Acc: 0.18557919621749408\n",
      "Epoch: 8 Train Loss: 2.4030918940058292, Valid Loss: 2.4219242290214256 |                   Train Acc: 0.19337670017740982, Valid Acc: 0.18439716312056736\n",
      "Epoch: 9 Train Loss: 2.3385695286516874, Valid Loss: 2.3722325607582375 |                   Train Acc: 0.19692489651094028, Valid Acc: 0.1867612293144208\n",
      "Epoch: 10 Train Loss: 2.302415020060989, Valid Loss: 2.3301862875620523 |                   Train Acc: 0.19396806623299823, Valid Acc: 0.18321513002364065\n",
      "Epoch: 11 Train Loss: 2.2758357187487044, Valid Loss: 2.3184314127321595 |                   Train Acc: 0.1992903607332939, Valid Acc: 0.18912529550827423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()  \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Training the model with cross validation on multiple epochs\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrain_multiple_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m, in \u001b[0;36mtrain_multiple_folds\u001b[1;34m(n_epochs, n_folds, batch_size, splits, writer, optimizer_class, optimizer_params, loss_fn)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Looping through all epochs\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs): \n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Training the model one epoch\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sampler_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Validating the model on one epoch\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validation_epoch(model, val_loader, loss_fn, val_sampler_size)\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, train_loader, loss_fn, train_sampler_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calculating the mask for the current position\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mgenerate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     24\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mask)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Moving inputs, labels and mask to the gpu/cpu\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calculating the mask for the current position\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m mask \u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m     24\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mask)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Moving inputs, labels and mask to the gpu/cpu\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mgenerate_mask\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m      7\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(tensor[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m6\u001b[39m], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# If layer 13 has any 1 it will be blacks turn\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Black pieces are in layers 6 to 11, summing across those layers\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(tensor[\u001b[38;5;241m6\u001b[39m:\u001b[38;5;241m12\u001b[39m], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Returning the masking in a list format\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 25  # Number of epochs\n",
    "BATCH_SIZE = 32  # Number of batches\n",
    "K = 3  # Number of folds\n",
    "\n",
    "# Logs training statistics for TensorBoard visualization\n",
    "writer = SummaryWriter(f\"runs/piece_to_move_{timestamp}\")  \n",
    "\n",
    "# Obtaining folds for cross validaiton\n",
    "splits = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# Initializing optimizer parameters to pass during training\n",
    "optimizer_class = optim.Adam\n",
    "optimizer_params = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-5\n",
    "}\n",
    "\n",
    "# Initializing the loss function\n",
    "# Cross entropy loss used since it is a classification\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "# Training the model with cross validation on multiple epochs\n",
    "train_multiple_folds(EPOCHS, K, BATCH_SIZE, splits, writer, optimizer_class, optimizer_params, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
