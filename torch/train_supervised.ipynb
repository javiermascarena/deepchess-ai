{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aux_functions\n",
    "import importlib\n",
    "\n",
    "importlib.reload(aux_functions)\n",
    "from aux_functions import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, SubsetRandomSampler\n",
    "# For masking\n",
    "from torch.masked import masked_tensor, as_masked_tensor\n",
    "\n",
    "import numpy as np\n",
    "import chess\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PROCESSING**\n",
    "\n",
    "- Importing the pgn data\n",
    "- Transforming the data to sparce tensors \n",
    "- Splitting the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845\n",
      "[414, 235, 1355, 1009, 437, 1064, 2105, 260, 2175, 81, 291, 79, 728, 2653, 2508, 1510, 2518, 2965, 2428, 1621, 2124, 763, 1293, 797, 1454, 1299, 443, 686, 967, 2616, 231, 283, 2045, 49, 2573, 717, 2412, 2931, 1205, 376, 678, 3326, 2258, 986, 1175, 861, 929, 1622, 3254, 3260, 3038, 408, 705, 1465, 949, 1752, 3331, 3113, 222, 365, 879, 128, 3065, 1309, 1241, 3233, 284, 2849, 2422, 2786, 1131, 1511, 92, 1568, 114, 2332, 2917, 1498, 1287, 1329, 126, 2037, 1326, 1383, 995, 1641, 3007, 3150, 293, 671, 2797, 206, 974, 3096, 2249, 229, 2462, 4, 2707, 1108, 35, 1747, 3098, 3055, 442, 1006, 2830, 2053, 393, 2667, 2633, 883, 1056, 1983, 1274, 1399, 3317, 100, 2969, 1744, 3337, 62, 1575, 2861, 1433, 715, 719, 2141, 2795, 2809, 446, 2584, 2959, 2831, 470, 1093, 2034, 381, 2264, 1448, 683, 1388, 1432, 1123, 1047, 941, 2257, 976, 959, 699, 790, 767, 2064, 2994, 2568, 2367, 833, 3168, 1400, 1325, 2240, 277, 2949, 1449, 2513, 3061, 1336, 1401, 3335, 290, 3339, 1367, 2817, 3114, 1415, 787, 2087, 2304, 464, 1535, 3207, 1555, 321, 2790, 2503, 1856, 1438, 2145, 3262, 2638, 1833, 157, 378, 2761, 1734, 1345, 1530, 2149, 2868, 113, 997, 1424, 2737, 2368, 2280, 1995, 1803, 1871, 2538, 2449, 1593, 546, 74, 3177, 2488, 3199, 1944, 3100, 2706, 1248, 2152, 1888, 2162, 880, 707, 118, 2838, 1370, 835, 3248, 1091, 1922, 178, 2765, 1642, 2050, 2101, 423, 711, 1363, 2564, 263, 1852, 2260, 2203, 730, 2711, 588, 354, 1285, 193, 5, 2741, 2605, 2603, 3013, 1343, 2825, 1038, 3330, 687, 2836, 2120, 108, 742, 803, 869, 1114, 3243, 2665, 2591, 1687, 110, 2792, 2918, 649, 1557, 3261, 1753, 794, 1919, 1070, 800, 1881, 496, 3156, 2068, 651, 1273, 1036, 481, 2522, 502, 2007, 574, 3304, 1928, 1238, 1889, 999, 3151, 2163, 2341, 1663, 2566, 2423, 1897, 150, 1083, 2853, 85, 799, 0, 2006, 3003, 3044, 2061, 227, 3223, 2799, 1969, 2743, 1201, 2912, 2110, 1397, 690, 702, 1855, 1138, 2020, 2271, 2179, 1001, 2421, 1910, 1573, 2338, 3062, 780, 192, 981, 2455, 3173, 2608, 2207, 1574, 3221, 238, 796, 2171, 335, 1647, 2130, 1786, 1422, 3026, 2217, 2215, 3220, 2880, 1271, 2794, 29, 2926, 266, 2858, 1916, 1313, 511, 3237, 975, 779, 2035, 3028, 724, 419, 2783, 920, 2443, 2371, 2673, 26, 1321, 737, 1227, 1194, 2049, 3291, 1368, 138, 2238, 1467, 115, 1525, 1451, 1289, 1954, 2916, 3107, 2898, 1514, 3180, 636, 2901, 83, 1223, 862, 1381, 647, 181, 2405, 1595, 1492, 2782, 899, 2637, 1931, 1661, 521, 3299, 367, 582, 3258, 2299, 1080, 1740, 627, 1968, 1067, 1706, 2977, 2553, 215, 3198, 2075, 1339, 71, 3121, 1119, 125, 2889, 120, 1214, 1215, 2279, 2195, 1746, 923, 59, 2402, 3023, 3300, 3006, 2450, 3120, 1317, 1409, 508, 205, 360, 2404, 898, 514, 40, 1104, 2013, 874, 2697, 3372, 830, 3205, 793, 352, 258, 572, 94, 2363, 1075, 372, 1820, 2316, 1666, 1469, 1304, 2625, 762, 296, 3285, 1173, 1133, 2177, 2407, 1630, 2139, 1042, 2081, 925, 2791, 2903, 847, 48, 2738, 2025, 3050, 105, 3249, 6, 1022, 2796, 479, 3086, 1658, 1960, 3090, 3174, 658, 2362, 95, 1659, 2546, 2659, 834, 628, 1233, 872, 480, 1826, 1506, 1633, 1802, 888, 3316, 622, 1324, 2694, 1775, 54, 2542, 449, 2624, 2144, 2733, 530, 1645, 2281, 2106, 1946, 854, 189, 1139, 812, 2798, 2690, 2302, 23, 494, 129, 2708, 63, 2689, 3200, 1503, 2235, 2243, 1305, 3025, 390, 1867, 2702, 727, 831, 829, 2159, 2684, 2172, 2283, 1668, 1731, 2397, 2248, 668, 2382, 1809, 2491, 2331, 435, 1695, 497, 2430, 2224, 2474, 1290, 384, 2014, 2501, 666, 695, 2976, 532, 820, 1189, 1117, 3166, 3141, 2067, 528, 1991, 659, 438, 341, 1724, 643, 1887, 1394, 1033, 2285, 2041, 816, 3265, 757, 3158, 2442, 1721, 2940, 1500, 300, 3184, 3381, 478, 2160, 1177, 602, 2655, 2364, 663, 662, 2319, 1134, 2307, 2343, 561, 950, 255, 3368, 2490, 3187, 2744, 556, 1517, 3183, 1008, 805, 2228, 2692, 3280, 3364, 1335, 3149, 1678, 597, 3085, 2498, 1172, 3338, 3127, 3109, 1442, 1589, 86, 564, 2517, 1027, 1804, 2259, 1697, 765, 2756, 585, 701, 1, 73, 3060, 1257, 1140, 3320, 1628, 3349, 2696, 2512, 2340, 1236, 823, 3247, 1801, 633, 450, 2077, 1069, 2748, 444, 2387, 2929, 2878, 452, 259, 679, 2908, 3363, 2146, 3169, 1736, 2389, 1078, 2718, 1974, 2329, 2122, 1213, 1518, 2759, 782, 1010, 1272, 422, 786, 198, 361, 1581, 2740, 581, 3269, 1196, 3345, 540, 1722, 462, 1031, 2473, 1037, 2310, 2230, 1452, 3322, 2444, 1102, 1406, 2620, 858, 817, 2070, 1224, 388, 2295, 2845, 1540, 1340, 463, 2859, 2820, 2439, 1959, 2424, 2005, 1703, 1089, 1605, 466, 1497, 3144, 1016, 571, 2828, 2826, 379, 915, 548, 2263, 485, 1167, 2214, 1839, 1636, 1461, 3088, 2985, 1726, 1192, 2127, 2019, 3352, 2857, 1019, 241, 980, 1463, 1456, 1615, 3070, 469, 2587, 3163, 1551, 2670, 1352, 1404, 1225, 1411, 2072, 554, 945, 1719, 1836, 2935, 1866, 304, 1296, 697, 673, 2115, 534, 2533, 2972, 1283, 2471, 2979, 1655, 234, 684, 2938, 3235, 3056, 1979, 1773, 130, 2209, 399, 3080, 2672, 3039, 338, 116, 1015, 2769, 2675, 1584, 2022, 340, 1254, 604, 2119, 2531, 568, 1987, 1212, 1264, 2018, 420, 2425, 610, 938, 3073, 505, 1592, 1891, 1681, 2393, 3234, 1094, 2652, 3024, 844, 2891, 303, 2647, 2398, 2763, 1239, 219, 2246, 1932, 166, 2735, 3239, 1745, 620, 2446, 1696, 380, 2816, 3042, 1121, 89, 2928, 1582, 2470, 216, 209, 2180, 1670, 2495, 982, 1873, 2460, 2520, 1446, 909, 1561, 832, 2016, 1314, 865, 2704, 897, 1798, 1197, 2919, 3045, 1842, 2457, 1132, 1813, 164, 594, 2255, 407, 2958, 2993, 3276, 3138, 3332, 1694, 172, 249, 183, 1905, 127, 1886, 3315, 2385, 3351, 605, 1649, 2071, 2987, 553, 2847, 1939, 1755, 3232, 2944, 1652, 1702, 2274, 21, 896, 2618, 1328, 2864, 2899, 1918, 1738, 3270, 842, 2688, 3328, 1311, 2126, 398, 808, 1603, 579, 212, 1816, 722, 2210, 3348, 3307, 1484, 1185, 906, 1493, 2459, 2932, 638, 2353, 2955, 12, 2651, 477, 2088, 2166, 887, 1827, 3192, 2896, 208, 1436, 3103, 1966, 507, 1221, 250, 1435, 838, 1524, 1505, 642, 1921, 2681, 1412, 2082, 1164, 603, 652, 1792, 2877, 621, 2960, 1964, 1504, 2167, 3240, 483, 537, 1757, 3196, 968, 1758, 1437, 3135, 2686, 3095, 676, 1077, 491, 3217, 11, 3146, 1149, 1112, 1818, 2900, 2187, 3301, 349, 18, 783, 436, 2392, 2663, 2881, 1228, 3010, 694, 7, 3370, 1560, 1103, 746, 934, 3289, 2266, 1784, 275, 2749, 343, 542, 2282, 3142, 1405, 1623, 370, 2586, 311, 1439, 357, 2888, 1426, 1539, 2311, 256, 1536, 947, 630, 1998, 1512, 1858, 640, 1190, 2805, 674, 2885, 721, 168, 392, 2360, 187, 1318, 1848, 580, 289, 1322, 596, 1198, 2284, 2504, 140, 50, 1519, 424, 2677, 1090, 1685, 1430, 525, 1612, 2669, 1136, 211, 2855, 165, 3188, 2722, 2352, 1030, 706, 156, 2909, 3030, 2876, 1768, 3185, 2256, 1817, 91, 1414, 1679, 1546, 774, 754, 1545, 1390, 2227, 2572, 639, 1307, 547, 268, 3082, 1578, 270, 123, 315, 484, 1346, 1220, 2793, 2410, 2322, 592, 2361, 245, 893, 1377, 2911, 2860, 1458, 455, 2556, 3342, 2465, 791, 856, 2760, 1677, 2807, 1182, 2497, 1606, 1028, 611, 242, 1181, 3091, 1174, 966, 1587, 415, 2923, 329, 3303, 1499, 1413, 617, 2514, 1914, 1260, 2479, 2357, 1298, 1877, 2551, 641, 2001, 244, 2438, 2986, 2234, 1129, 1608, 90, 1815, 1005, 1682, 3153, 2770, 2391, 1058, 1654, 428, 2927, 988, 2244, 2485, 618, 2674, 1614, 942, 958, 1478, 1071, 1926, 2326, 1186, 41, 2950, 713, 1883, 2108, 2806, 3182, 849, 905, 2775, 3134, 1391, 2486, 3078, 855, 2445, 1294, 2189, 2038, 201, 1098, 710, 3216, 3122, 1794, 1043, 1419, 8, 2480, 1532, 1594, 3273, 1126, 121, 644, 2121, 836, 1920, 3170, 2821, 1885, 3077, 1844, 2597, 1348, 2930, 1051, 3057, 2002, 784, 770, 769, 3019, 619, 3047, 2656, 1473, 33, 308, 1332, 1978, 1007, 3041, 332, 810, 1812, 153, 519, 93, 853, 886, 171, 2867, 1230, 1124, 2839, 2649, 1086, 328, 1674, 1199, 1195, 1060, 1002, 2784, 2593, 3257, 1814, 598, 1249, 2636, 2117, 1710, 1543, 2017, 2140, 2048, 2487, 3005, 1947, 2823, 682, 2990, 1997, 279, 1846, 1548, 204, 1331, 747, 2639, 3112, 1643, 1558, 1269, 2231, 1847, 1763, 859, 601, 3020, 826, 1534, 1143, 1896, 2309, 2724, 2345, 1680, 1231, 2602, 72, 1971, 1088, 1788, 1723, 522, 3167, 3309, 1369, 1996, 1247, 751, 2024, 2043, 3227, 2245, 2511, 903, 1531, 660, 1875, 1113, 545, 1849, 2968, 2946, 3022, 44, 112, 400, 1748, 1901, 1577, 1495, 3283, 1952, 2808, 1222, 2730, 3323, 3319, 1992, 2289, 3058, 623, 2532, 3308, 2118, 133, 2074, 102, 667, 471, 1443, 2601, 2489, 184, 2519, 3347, 1250, 1474, 1163, 889, 1904, 589, 2003, 1835, 2524, 1347, 612, 3037, 570, 151, 3129, 3350, 1356, 3087, 334, 1878, 64, 2612, 3253, 1320, 2031, 441, 3105, 3115, 2301, 185, 2496, 3094, 47, 1772, 1382, 3357, 2493, 88, 3252, 1750, 573, 3246, 281, 635, 1384, 2057, 1529, 1045, 2771, 1930, 2804, 3292, 265, 109, 3313, 3251, 2206, 1973, 2313, 2543, 2176, 373, 996, 292, 2713, 517, 3324, 1242, 2196, 1785, 2335, 2194, 1263, 1183, 2237, 1559, 1202, 2296, 2716, 1771, 2153, 3213, 1154, 1880, 2824, 2865, 821, 586, 80, 1488, 3191, 857, 3376, 1796, 1542, 1756, 509, 3101, 1243, 2337, 1764, 3089, 410, 39, 2330, 2609, 257, 919, 2056, 2635, 3354, 656, 1934, 2646, 2286, 2535, 2787, 1462, 2200, 3297, 2600, 3011, 1158, 374, 492, 2613, 1105, 569, 3356, 3190, 2592, 2178, 13, 595, 809, 14, 1207, 2844, 1470, 3255, 1278, 804, 900, 2942, 3033, 1638, 1629, 1845, 918, 1127, 3333, 1585, 188, 2441, 1020, 30, 3362, 225, 1166, 2948, 1170, 1541, 2135, 2317, 1993, 2561, 3084, 560, 529, 213, 1913, 3128, 3035, 1279, 1000, 36, 1156, 371, 2028, 2507, 1725, 2400, 2347, 2112, 1076, 2712, 1829, 467, 2190, 2879, 2666, 1787, 1619, 2358, 2009, 2150, 324, 1712, 2774, 3126, 700, 819, 1011, 1096, 421, 901, 761, 2643, 2634, 1571, 1936, 369, 96, 2606, 1893, 1565, 2395, 2789, 3334, 1762, 445, 3092, 318, 846, 1097, 549, 1733, 2023, 418, 2978, 273, 2872, 877, 3202, 412, 3195, 827, 2086, 1057, 131, 3225, 3310, 788, 1689, 218, 2435, 224, 1471, 1657, 624, 2419, 2089, 2500, 646, 262, 1556, 2842, 1970, 3004, 2988, 2272, 495, 356, 401, 3178, 3374, 2750, 1403, 2550, 176, 1791, 145, 1950, 37, 523, 382, 1065, 1150, 1933, 2373, 2526, 1316, 1523, 1253, 175, 2660, 1945, 1044, 243, 1165, 1082, 1387, 2569, 425, 3256, 1783, 501, 1599, 331, 161, 362, 1300, 921, 1728, 1553, 2211, 2472, 267, 2945, 2275, 655, 2411, 3139, 2776, 931, 3228, 2574, 2662, 1927, 1427, 3355, 868, 3081, 1282, 1562, 1912, 1393, 1537, 2137, 2802, 894, 1188, 1691, 1607, 2342, 773, 386, 2902, 2915, 1683, 3108, 248, 2406, 3076, 197, 2156, 1665, 973, 518, 2218, 2641, 2481, 316, 347, 1074, 1502, 2679, 1963, 297, 2113, 908, 802, 389, 3293, 1203, 1402, 2456, 1344, 2454, 1507, 2952, 965, 1860, 1554, 1152, 403, 2351, 1380, 27, 1770, 233, 190, 3099, 3136, 1509, 2980, 2063, 2621, 144, 2590, 498, 933, 1048, 927, 2850, 972, 475, 1361, 317, 426, 2133, 1211, 56, 1705, 38, 1145, 1859, 2974, 1464, 306, 881, 1776, 239, 448, 1717, 2094, 269, 2314, 2223, 1245, 1032, 1962, 3241, 1444, 2710, 696, 1206, 3272, 186, 634, 2381, 2866, 2107, 1162, 2714, 1237, 2212, 2109, 1874, 1800, 648, 1244, 1395, 2727, 1876, 3305, 34, 3336, 2982, 1735, 2085, 3179, 2293, 1781, 2452, 3008, 2267, 1226, 998, 3130, 2680, 1389, 1549, 504, 298, 1729, 2615, 3340, 1870, 1385, 427, 2241, 1365, 1204, 1868, 2193, 2970, 2871, 2440, 1739, 2339, 2000, 1479, 119, 203, 952, 2719, 2197, 629, 1994, 951, 2492, 1806, 1256, 2093, 261, 1081, 1341, 3380, 1455, 3193, 2966, 1334, 2379, 677, 814, 240, 1410, 2510, 194, 254, 2102, 3208, 3093, 2736, 845, 3377, 1564, 2396, 2562, 2142, 562, 2732, 755, 375, 1200, 689, 274, 3379, 1161, 2394, 1778, 3290, 302, 2614, 2494, 1417, 2065, 2567, 593, 813, 312, 405, 943, 734, 1972, 2910, 2409, 1440, 1486, 2188, 3137, 1379, 3119, 2715, 3160, 818, 1251, 971, 199, 1598, 3110, 1583, 3054, 3244, 1648, 3155, 778, 1106, 2922, 2463, 1235, 987, 2219, 510, 503, 1567, 1713, 489, 223, 3036, 2202, 625, 567, 3124, 961, 2701, 1808, 1650, 2432, 2557, 327, 252, 77, 1637, 2907, 1333, 1949, 2906, 1487, 87, 3302, 1496, 3157, 1988, 3373, 3219, 3002, 653, 104, 155, 2216, 1720, 1841, 1084, 637, 355, 3294, 2983, 1751, 714, 1692, 1447, 2033, 2873, 1906, 922, 807, 1366, 1782, 2242, 3067, 2205, 416, 2619, 2971, 789, 680, 174, 68, 2116, 1823, 60, 1258, 870, 2962, 246, 2015, 170, 3052, 348, 2788, 98, 1398, 2552, 2303, 251, 1407, 940, 2540, 1281, 2883, 2011, 979, 924, 2288, 1632, 2747, 1671, 2992, 1828, 3346, 795, 775, 447, 2664, 1900, 2981, 1759, 2728, 1899, 760, 1327, 608, 613, 217, 937, 3097, 3014, 2261, 2581, 17, 1766, 111, 913, 305, 2897, 1547, 1216, 2333, 985, 1441, 141, 1323, 32, 143, 1700, 541, 2098, 2627, 2174, 1276, 840, 2039, 2298, 1476, 2466, 2170, 3365, 31, 3189, 2386, 1769, 3329, 1291, 202, 2604, 3282, 2370, 2458, 2699, 708, 2164, 2964, 1662, 1869, 2328, 1653, 1137, 1209, 2525, 2273, 2226, 2623, 738, 2780, 1805, 322, 57, 278, 2811, 1831, 2051, 1286, 2408, 299, 607, 2645, 103, 3375, 2575, 2943, 2161, 2278, 2004, 2377, 3186, 1981, 2420, 2469, 977, 1151, 2374, 1109, 1308, 536, 1790, 2703, 693, 2506, 1135, 160, 55, 2058, 2073, 1265, 3366, 2698, 939, 2390, 914, 101, 20, 1942, 645, 891, 3051, 288, 2012, 935, 851, 2822, 1171, 1990, 1989, 2521, 2165, 1596, 2453, 122, 2434, 2268, 1570, 180, 1693, 1902, 1386, 2173, 2198, 1627, 1908, 2937, 506, 3, 1576, 3029, 863, 1040, 792, 954, 2729, 1364, 962, 309, 1984, 2059, 1148, 2995, 1099, 1765, 402, 173, 58, 3209, 2963, 1741, 3040, 2325, 2598, 928, 2251, 2843, 3268, 169, 2100, 2837, 1837, 2785, 3059, 2192, 2300, 3165, 2431, 2042, 1358, 2676, 2290, 2467, 1861, 2626, 716, 383, 1085, 2848, 413, 3145, 1374, 2097, 1445, 1618, 2813, 1911, 3116, 3281, 3222, 2233, 1457, 1302, 2306, 723, 1797, 2182, 2157, 2052, 1951, 433, 1092, 2447, 1023, 1141, 1610, 712, 3162, 1879, 2545, 377, 1193, 1854, 3321, 1580, 917, 1843, 2835, 177, 2893, 313, 1810, 1408, 2800, 1631, 2375, 2383, 196, 2476, 1591, 2415, 815, 1046, 353, 2292, 1999, 1982, 843, 2695, 1420, 3312, 142, 969, 1626, 3001, 2047, 9, 301, 1714, 1303, 76, 1059, 3369, 1012, 1354, 1466, 16, 768, 1675, 1777, 551, 616, 3066, 1280, 1115, 1634, 490, 226, 3152, 1651, 781, 2869, 2348, 1516, 1915, 3147, 806, 824, 672, 2734, 650, 465, 2709, 1246, 1351, 1315, 563, 2577, 2477, 1485, 1850, 2854, 404, 191, 1359, 1985, 1604, 895, 1353, 339, 1477, 990, 2247, 910, 2380, 2754, 878, 2642, 1644, 2726, 1004, 319, 2478, 1483, 2095, 731, 732, 1538, 2482, 237, 429, 70, 2658, 1144, 3009, 468, 685, 124, 3360, 748, 15, 453, 1853, 1360, 2607, 1822, 2725, 1660, 1701, 726, 2555, 1482, 2953, 3361, 2232, 3172, 1882, 956, 2904, 2818, 2921, 3074, 2417, 735, 1130, 1295, 1521, 1396, 159, 609, 3259, 3131, 99, 1563, 1862, 2044, 221, 512, 3306, 2815, 2851, 1673, 871, 2384, 2148, 1068, 3181, 575, 3210, 1625, 1372, 162, 2700, 739, 3245, 2327, 2104, 207, 615, 1475, 1948, 3250, 957, 3236, 42, 2947, 3083, 2829, 2403, 2571, 1063, 2693, 675, 2516, 460, 487, 1749, 2599, 670, 688, 3275, 2, 2132, 2451, 2611, 134, 1072]\n"
     ]
    }
   ],
   "source": [
    "TEST_PERCENT = 0.25\n",
    "\n",
    "# Load pgn paths\n",
    "pgns = import_data(5)\n",
    "\n",
    "# Convert pgns to tensors\n",
    "board_tensors, next_moves = parse_pgn_to_tensors(pgns)\n",
    "\n",
    "# Converting the dataset into a custom pytorch one\n",
    "dataset = ChessDataset(board_tensors, next_moves)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Splitting the data into train and test\n",
    "train_dataset, test_data = torch.utils.data.random_split(dataset, [1-TEST_PERCENT, TEST_PERCENT])\n",
    "\n",
    "print(len(test_data))  # Number of states\n",
    "print(train_dataset.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEURAL NETWORK DESIGN**\n",
    "- 2 Convolutional layers\n",
    "- 2 Fully connected hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to do the operations on the cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PieceToMoveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Takes as input a tensor of 14 channels (8x8 board)\n",
    "        self.conv1 = nn.Conv2d(14, 6, 3)  # 6 filters, 3x3 kernel\n",
    "        self.pool = nn.MaxPool2d(2, 2)    # Max pooling with 2x2 window\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)  # 16 filters, 3x3 kernel\n",
    "\n",
    "        # Using droput to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        # Using batch normalization to make training faster and more stable\n",
    "        self.bn1 = nn.BatchNorm1d(120)  # For the 1st layer\n",
    "        self.bn2 = nn.BatchNorm1d(84)   # For the 2nd layer\n",
    "        \n",
    "        # Output from conv2 will be (16 channels, 1x1 feature maps)\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # Predicts the tile to move the piece from (64 possible tiles on the board)\n",
    "        self.fc3 = nn.Linear(84, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Apply first conv + pooling\n",
    "        x = F.relu(self.conv2(x))             # Apply second conv to get (16 x 1 x 1)\n",
    "        x = torch.flatten(x, 1)               # Flatten all dimensions except batch size\n",
    "        x = F.relu(self.bn1(self.fc1(x)))     # Fully connected layer 1 and batch normalization\n",
    "        x = self.dropout(x)                   # Dropout of some first layer neurons\n",
    "        x = F.relu(self.bn2(self.fc2(x)))     # Fully connected layer 2\n",
    "        x = self.fc3(x)                       # Output layer (no activation, logits for classification)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initializing the network\n",
    "piece_to_move_net = PieceToMoveNet()\n",
    "# Move the network to gpu/cpu befor initializing the optimizer\n",
    "piece_to_move_net.to(device)\n",
    "\n",
    "# Adam optimizer will be used due to its versatility\n",
    "optimizer = optim.Adam(piece_to_move_net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING LOOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javie\\deepchess-ai\\torch\\aux_functions.py:194: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(tensor, dtype=torch.float32), from_pos_tensor, to_pos_tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 3.8770342071851096, Valid Loss: 3.833964556455612 | Train Acc: 0.12045554095488392, Valid Acc: 0.12992125984251968\n",
      "Epoch: 1 Train Loss: 2.9356159567832947, Valid Loss: 2.017421677708626 | Train Acc: 0.2378449408672799, Valid Acc: 0.28346456692913385\n",
      "Epoch: 2 Train Loss: 1.8286440438694425, Valid Loss: 1.7823016792535782 | Train Acc: 0.29172141918528255, Valid Acc: 0.2677165354330709\n",
      "Epoch: 3 Train Loss: 1.7420025633441076, Valid Loss: 1.7310683131217957 | Train Acc: 0.29916776171703896, Valid Acc: 0.28346456692913385\n",
      "Epoch: 4 Train Loss: 1.6996687998374302, Valid Loss: 1.7140476405620575 | Train Acc: 0.31756460797196673, Valid Acc: 0.2874015748031496\n",
      "Epoch: 5 Train Loss: 1.6809568487935596, Valid Loss: 1.6891295909881592 | Train Acc: 0.31975470871660094, Valid Acc: 0.2874015748031496\n",
      "Epoch: 6 Train Loss: 1.6494291772445042, Valid Loss: 1.6799979656934738 | Train Acc: 0.328515111695138, Valid Acc: 0.3110236220472441\n",
      "Epoch: 7 Train Loss: 1.6353432089090347, Valid Loss: 1.6593640446662903 | Train Acc: 0.3320192728865528, Valid Acc: 0.2874015748031496\n",
      "Epoch: 8 Train Loss: 1.6227725760804281, Valid Loss: 1.649250566959381 | Train Acc: 0.3311432325886991, Valid Acc: 0.3031496062992126\n",
      "Epoch: 9 Train Loss: 1.6074333803521261, Valid Loss: 1.6427705734968185 | Train Acc: 0.334209373631187, Valid Acc: 0.30708661417322836\n",
      "FOLD 2\n",
      "Epoch: 0 Train Loss: 1.6015028837654326, Valid Loss: 1.5851260125637054 | Train Acc: 0.34734997809899254, Valid Acc: 0.33070866141732286\n",
      "Epoch: 1 Train Loss: 1.5892605781555176, Valid Loss: 1.5506489723920822 | Train Acc: 0.3320192728865528, Valid Acc: 0.36220472440944884\n",
      "Epoch: 2 Train Loss: 1.578155689769321, Valid Loss: 1.560446873307228 | Train Acc: 0.3442838370565046, Valid Acc: 0.33858267716535434\n",
      "Epoch: 3 Train Loss: 1.5652483138773177, Valid Loss: 1.5675800144672394 | Train Acc: 0.34472185720543147, Valid Acc: 0.3464566929133858\n",
      "Epoch: 4 Train Loss: 1.5628580500682194, Valid Loss: 1.55454920232296 | Train Acc: 0.34734997809899254, Valid Acc: 0.3700787401574803\n",
      "Epoch: 5 Train Loss: 1.548033959335751, Valid Loss: 1.58218514919281 | Train Acc: 0.35216819973718794, Valid Acc: 0.31496062992125984\n",
      "Epoch: 6 Train Loss: 1.53662581079536, Valid Loss: 1.5740249305963516 | Train Acc: 0.35348226018396844, Valid Acc: 0.3425196850393701\n",
      "Epoch: 7 Train Loss: 1.5284742150041792, Valid Loss: 1.54517762362957 | Train Acc: 0.3679369250985545, Valid Acc: 0.38976377952755903\n",
      "Epoch: 8 Train Loss: 1.5242729004886415, Valid Loss: 1.55827097594738 | Train Acc: 0.3609286027157249, Valid Acc: 0.35039370078740156\n",
      "Epoch: 9 Train Loss: 1.5191855596171484, Valid Loss: 1.5620976984500885 | Train Acc: 0.3661848445028471, Valid Acc: 0.35826771653543305\n",
      "FOLD 3\n",
      "Epoch: 0 Train Loss: 1.5104261404938168, Valid Loss: 1.5272410959005356 | Train Acc: 0.3670608848007008, Valid Acc: 0.37401574803149606\n",
      "Epoch: 1 Train Loss: 1.5004251814550824, Valid Loss: 1.5317551493644714 | Train Acc: 0.3731931668856767, Valid Acc: 0.3779527559055118\n",
      "Epoch: 2 Train Loss: 1.4979610194762547, Valid Loss: 1.5404477566480637 | Train Acc: 0.3793254489706526, Valid Acc: 0.3188976377952756\n",
      "Epoch: 3 Train Loss: 1.4927399439944162, Valid Loss: 1.5320717990398407 | Train Acc: 0.3745072273324573, Valid Acc: 0.3700787401574803\n",
      "Epoch: 4 Train Loss: 1.4881560040844812, Valid Loss: 1.5213046371936798 | Train Acc: 0.38020148926850633, Valid Acc: 0.37401574803149606\n",
      "Epoch: 5 Train Loss: 1.4730807228220835, Valid Loss: 1.5486661046743393 | Train Acc: 0.38501971090670173, Valid Acc: 0.3346456692913386\n",
      "Epoch: 6 Train Loss: 1.4680942412879732, Valid Loss: 1.5264980792999268 | Train Acc: 0.38501971090670173, Valid Acc: 0.36220472440944884\n",
      "Epoch: 7 Train Loss: 1.4652694811423619, Valid Loss: 1.5281485170125961 | Train Acc: 0.3854577310556286, Valid Acc: 0.35039370078740156\n",
      "Epoch: 8 Train Loss: 1.4571812682681613, Valid Loss: 1.541725531220436 | Train Acc: 0.39509417433201927, Valid Acc: 0.32677165354330706\n",
      "Epoch: 9 Train Loss: 1.449717351131969, Valid Loss: 1.5250884741544724 | Train Acc: 0.3937801138852387, Valid Acc: 0.3661417322834646\n",
      "FOLD 4\n",
      "Epoch: 0 Train Loss: 1.4462382164266374, Valid Loss: 1.474672257900238 | Train Acc: 0.3907139728427508, Valid Acc: 0.4251968503937008\n",
      "Epoch: 1 Train Loss: 1.4535323546992407, Valid Loss: 1.521907314658165 | Train Acc: 0.38896189224704336, Valid Acc: 0.38976377952755903\n",
      "Epoch: 2 Train Loss: 1.440356155236562, Valid Loss: 1.5130640268325806 | Train Acc: 0.3946561541830924, Valid Acc: 0.38976377952755903\n",
      "Epoch: 3 Train Loss: 1.427354771229956, Valid Loss: 1.5391862392425537 | Train Acc: 0.40254051686377573, Valid Acc: 0.3700787401574803\n",
      "Epoch: 4 Train Loss: 1.426775402492947, Valid Loss: 1.5333003848791122 | Train Acc: 0.401664476565922, Valid Acc: 0.3779527559055118\n",
      "Epoch: 5 Train Loss: 1.420117022262679, Valid Loss: 1.5240933746099472 | Train Acc: 0.3999123959702146, Valid Acc: 0.3937007874015748\n",
      "Epoch: 6 Train Loss: 1.4203704595565796, Valid Loss: 1.5307932794094086 | Train Acc: 0.40954883924660535, Valid Acc: 0.38976377952755903\n",
      "Epoch: 7 Train Loss: 1.409212913778093, Valid Loss: 1.532756581902504 | Train Acc: 0.4161191414805081, Valid Acc: 0.40551181102362205\n",
      "Epoch: 8 Train Loss: 1.3998306393623352, Valid Loss: 1.5453575998544693 | Train Acc: 0.41874726237406923, Valid Acc: 0.38976377952755903\n",
      "Epoch: 9 Train Loss: 1.4028349320093791, Valid Loss: 1.533160388469696 | Train Acc: 0.41874726237406923, Valid Acc: 0.38976377952755903\n",
      "FOLD 5\n",
      "Epoch: 0 Train Loss: 1.4161199811432097, Valid Loss: 1.3037090748548508 | Train Acc: 0.40779675865089793, Valid Acc: 0.48031496062992124\n",
      "Epoch: 1 Train Loss: 1.4117297993765936, Valid Loss: 1.3238259553909302 | Train Acc: 0.401664476565922, Valid Acc: 0.44881889763779526\n",
      "Epoch: 2 Train Loss: 1.4070676962534587, Valid Loss: 1.335925281047821 | Train Acc: 0.4191852825229961, Valid Acc: 0.43700787401574803\n",
      "Epoch: 3 Train Loss: 1.4036573270956676, Valid Loss: 1.3175173699855804 | Train Acc: 0.4130530004380201, Valid Acc: 0.452755905511811\n",
      "Epoch: 4 Train Loss: 1.3991745710372925, Valid Loss: 1.3452688604593277 | Train Acc: 0.42268944371441086, Valid Acc: 0.4094488188976378\n",
      "Epoch: 5 Train Loss: 1.38884141544501, Valid Loss: 1.3360711336135864 | Train Acc: 0.42706964520367935, Valid Acc: 0.43700787401574803\n",
      "Epoch: 6 Train Loss: 1.3851329038540523, Valid Loss: 1.3363089263439178 | Train Acc: 0.4231274638633377, Valid Acc: 0.4330708661417323\n",
      "Epoch: 7 Train Loss: 1.3804597523477342, Valid Loss: 1.3324753940105438 | Train Acc: 0.42268944371441086, Valid Acc: 0.4330708661417323\n",
      "Epoch: 8 Train Loss: 1.3785312854581409, Valid Loss: 1.3578428775072098 | Train Acc: 0.42882172579938677, Valid Acc: 0.4448818897637795\n",
      "Epoch: 9 Train Loss: 1.3795073413186603, Valid Loss: 1.3436955213546753 | Train Acc: 0.42093736311870344, Valid Acc: 0.43700787401574803\n",
      "FOLD 6\n",
      "Epoch: 0 Train Loss: 1.3792634622918234, Valid Loss: 1.3023873418569565 | Train Acc: 0.4323258869908016, Valid Acc: 0.4448818897637795\n",
      "Epoch: 1 Train Loss: 1.3741979317532644, Valid Loss: 1.2934273034334183 | Train Acc: 0.4393342093736312, Valid Acc: 0.47244094488188976\n",
      "Epoch: 2 Train Loss: 1.3670566529035568, Valid Loss: 1.302251473069191 | Train Acc: 0.42356548401226457, Valid Acc: 0.4566929133858268\n",
      "Epoch: 3 Train Loss: 1.3597544754544895, Valid Loss: 1.2943457663059235 | Train Acc: 0.43013578624616733, Valid Acc: 0.4645669291338583\n",
      "Epoch: 4 Train Loss: 1.3571985628869798, Valid Loss: 1.2944837659597397 | Train Acc: 0.43626806833114323, Valid Acc: 0.4566929133858268\n",
      "Epoch: 5 Train Loss: 1.3483419252766504, Valid Loss: 1.3036206513643265 | Train Acc: 0.4529128339903636, Valid Acc: 0.452755905511811\n",
      "Epoch: 6 Train Loss: 1.3424485623836517, Valid Loss: 1.301982045173645 | Train Acc: 0.4551029347349978, Valid Acc: 0.4566929133858268\n",
      "Epoch: 7 Train Loss: 1.3427065718505118, Valid Loss: 1.3103229999542236 | Train Acc: 0.45860709592641263, Valid Acc: 0.421259842519685\n",
      "Epoch: 8 Train Loss: 1.3415236903561487, Valid Loss: 1.3195614516735077 | Train Acc: 0.4480946123521682, Valid Acc: 0.44881889763779526\n",
      "Epoch: 9 Train Loss: 1.3331332736545138, Valid Loss: 1.3187458962202072 | Train Acc: 0.45554095488392465, Valid Acc: 0.43700787401574803\n",
      "FOLD 7\n",
      "Epoch: 0 Train Loss: 1.3289196607139375, Valid Loss: 1.3691784292459488 | Train Acc: 0.44634253175646077, Valid Acc: 0.4566929133858268\n",
      "Epoch: 1 Train Loss: 1.315377993716134, Valid Loss: 1.3820543438196182 | Train Acc: 0.4529128339903636, Valid Acc: 0.484251968503937\n",
      "Epoch: 2 Train Loss: 1.3095775445302327, Valid Loss: 1.3794844597578049 | Train Acc: 0.45729303547963207, Valid Acc: 0.44881889763779526\n",
      "Epoch: 3 Train Loss: 1.3125154044893053, Valid Loss: 1.3992696702480316 | Train Acc: 0.45466491458607095, Valid Acc: 0.4409448818897638\n",
      "Epoch: 4 Train Loss: 1.3040780425071716, Valid Loss: 1.408817544579506 | Train Acc: 0.46254927726675427, Valid Acc: 0.4645669291338583\n",
      "Epoch: 5 Train Loss: 1.2995967252386942, Valid Loss: 1.405377134680748 | Train Acc: 0.4682435392028033, Valid Acc: 0.452755905511811\n",
      "Epoch: 6 Train Loss: 1.301117006275389, Valid Loss: 1.417798325419426 | Train Acc: 0.46692947875602275, Valid Acc: 0.468503937007874\n",
      "Epoch: 7 Train Loss: 1.2831313494178984, Valid Loss: 1.4326985031366348 | Train Acc: 0.47525186158563293, Valid Acc: 0.4330708661417323\n",
      "Epoch: 8 Train Loss: 1.2872301240762074, Valid Loss: 1.4417306631803513 | Train Acc: 0.472185720543145, Valid Acc: 0.43700787401574803\n",
      "Epoch: 9 Train Loss: 1.2749166877733336, Valid Loss: 1.42633256316185 | Train Acc: 0.47262374069207186, Valid Acc: 0.452755905511811\n",
      "FOLD 8\n",
      "Epoch: 0 Train Loss: 1.2934918486409717, Valid Loss: 1.2630988508462906 | Train Acc: 0.4851138353765324, Valid Acc: 0.4782608695652174\n",
      "Epoch: 1 Train Loss: 1.2881901893350813, Valid Loss: 1.2792368531227112 | Train Acc: 0.48336252189141854, Valid Acc: 0.4426877470355731\n",
      "Epoch: 2 Train Loss: 1.28063855237431, Valid Loss: 1.2750193625688553 | Train Acc: 0.4754816112084063, Valid Acc: 0.45454545454545453\n",
      "Epoch: 3 Train Loss: 1.2818417996168137, Valid Loss: 1.274756297469139 | Train Acc: 0.48248686514886163, Valid Acc: 0.45849802371541504\n",
      "Epoch: 4 Train Loss: 1.283261425793171, Valid Loss: 1.3250356614589691 | Train Acc: 0.47285464098073554, Valid Acc: 0.42292490118577075\n",
      "Epoch: 5 Train Loss: 1.2708410256438785, Valid Loss: 1.3135195076465607 | Train Acc: 0.48029772329246934, Valid Acc: 0.4268774703557312\n",
      "Epoch: 6 Train Loss: 1.2679908391502168, Valid Loss: 1.3027425408363342 | Train Acc: 0.47504378283712784, Valid Acc: 0.4426877470355731\n",
      "Epoch: 7 Train Loss: 1.2616527982883983, Valid Loss: 1.3019734472036362 | Train Acc: 0.4903677758318739, Valid Acc: 0.4308300395256917\n",
      "Epoch: 8 Train Loss: 1.2530433866712782, Valid Loss: 1.3172406554222107 | Train Acc: 0.4925569176882662, Valid Acc: 0.4150197628458498\n",
      "Epoch: 9 Train Loss: 1.2563869696524408, Valid Loss: 1.3291113525629044 | Train Acc: 0.4960595446584939, Valid Acc: 0.4150197628458498\n",
      "FOLD 9\n",
      "Epoch: 0 Train Loss: 1.2636025440361764, Valid Loss: 1.2058239877223969 | Train Acc: 0.4925569176882662, Valid Acc: 0.49407114624505927\n",
      "Epoch: 1 Train Loss: 1.2669517174363136, Valid Loss: 1.2164781838655472 | Train Acc: 0.4868651488616462, Valid Acc: 0.5059288537549407\n",
      "Epoch: 2 Train Loss: 1.2517217778497272, Valid Loss: 1.2193296775221825 | Train Acc: 0.5, Valid Acc: 0.48221343873517786\n",
      "Epoch: 3 Train Loss: 1.2505433989895716, Valid Loss: 1.2008388489484787 | Train Acc: 0.4969352014010508, Valid Acc: 0.4901185770750988\n",
      "Epoch: 4 Train Loss: 1.246324587199423, Valid Loss: 1.2373483628034592 | Train Acc: 0.4956217162872154, Valid Acc: 0.47035573122529645\n",
      "Epoch: 5 Train Loss: 1.2452106037073665, Valid Loss: 1.2467828392982483 | Train Acc: 0.489492119089317, Valid Acc: 0.4743083003952569\n",
      "Epoch: 6 Train Loss: 1.2454667530126042, Valid Loss: 1.240975871682167 | Train Acc: 0.4960595446584939, Valid Acc: 0.43873517786561267\n",
      "Epoch: 7 Train Loss: 1.2402363659607039, Valid Loss: 1.224889874458313 | Train Acc: 0.4986865148861646, Valid Acc: 0.4624505928853755\n",
      "Epoch: 8 Train Loss: 1.2317513037059042, Valid Loss: 1.2295283377170563 | Train Acc: 0.5087565674255692, Valid Acc: 0.4901185770750988\n",
      "Epoch: 9 Train Loss: 1.232829152709908, Valid Loss: 1.2695548832416534 | Train Acc: 0.5074430823117339, Valid Acc: 0.43873517786561267\n",
      "FOLD 10\n",
      "Epoch: 0 Train Loss: 1.2290799534983106, Valid Loss: 1.2194393426179886 | Train Acc: 0.4995621716287215, Valid Acc: 0.5138339920948617\n",
      "Epoch: 1 Train Loss: 1.2249528169631958, Valid Loss: 1.2346206456422806 | Train Acc: 0.4995621716287215, Valid Acc: 0.5138339920948617\n",
      "Epoch: 2 Train Loss: 1.2143253692322307, Valid Loss: 1.2440366297960281 | Train Acc: 0.5074430823117339, Valid Acc: 0.4980237154150198\n",
      "Epoch: 3 Train Loss: 1.2153334211972024, Valid Loss: 1.2333953529596329 | Train Acc: 0.5118213660245184, Valid Acc: 0.5019762845849802\n",
      "Epoch: 4 Train Loss: 1.2091532771786053, Valid Loss: 1.2401743084192276 | Train Acc: 0.5109457092819615, Valid Acc: 0.48221343873517786\n",
      "Epoch: 5 Train Loss: 1.2047377758555942, Valid Loss: 1.2502255141735077 | Train Acc: 0.5118213660245184, Valid Acc: 0.49407114624505927\n",
      "Epoch: 6 Train Loss: 1.19695825709237, Valid Loss: 1.2688884288072586 | Train Acc: 0.5157618213660246, Valid Acc: 0.4980237154150198\n",
      "Epoch: 7 Train Loss: 1.1907969415187836, Valid Loss: 1.2783926278352737 | Train Acc: 0.5157618213660246, Valid Acc: 0.4743083003952569\n",
      "Epoch: 8 Train Loss: 1.1941754362649388, Valid Loss: 1.2634779810905457 | Train Acc: 0.5166374781085814, Valid Acc: 0.4505928853754941\n",
      "Epoch: 9 Train Loss: 1.1895632205737963, Valid Loss: 1.2821041494607925 | Train Acc: 0.5205779334500875, Valid Acc: 0.48616600790513836\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, loss_fn, train_sampler_size): \n",
    "    \"\"\"\n",
    "    Trains the model for one epoch and returns the average training loss and accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    running_loss = 0.  \n",
    "    running_correct = 0.\n",
    "\n",
    "    # Looping through all samples in a batch\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Extracting the board tensor\n",
    "        inputs = data[0]\n",
    "        # Extracting the tile of the piece to move\n",
    "        labels = data[1][:, 0]\n",
    "        \n",
    "        # Moving inputs and labels to the gpu/cpu\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Resetting the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Calculating model's output\n",
    "        outputs = model(inputs)\n",
    "        # Calculating the sample loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Calculating the gradient\n",
    "        loss.backward()\n",
    "        # Updating model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adding the last loss to the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate number of correct predictions\n",
    "        _, predictions = torch.max(outputs.data, 1)\n",
    "        running_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    # Averaging the loss for all samples in the batch\n",
    "    running_loss /= (i + 1)\n",
    "\n",
    "    # Calculate accuracy based on the total samples in the fold (train_sampler_size)\n",
    "    train_accuracy = running_correct / train_sampler_size\n",
    "\n",
    "    return running_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validation_epoch(model, validation_loader, loss_fn, val_sampler_size):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch and returns the average validation loss and accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    running_vloss = 0.\n",
    "    running_vcorrect = 0.\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations for validation set\n",
    "    with torch.no_grad():\n",
    "        # Looping through all batches in the validation set\n",
    "        for i, v_data in enumerate(validation_loader):\n",
    "            # Getting the tensors of the validation data\n",
    "            vinputs = v_data[0]\n",
    "            vlabels = v_data[1][:, 0]\n",
    "\n",
    "            # Moving inputs and labels to the gpu/cpu\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "\n",
    "            # Calculating the output of the model\n",
    "            voutputs = model(vinputs)\n",
    "            # Calculating the loss of the model in the validation sample\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            # Adding this sample's loss to the total loss\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "            # Calculate number of correct predictions\n",
    "            _, predictions = torch.max(voutputs.data, 1)\n",
    "            running_vcorrect += (predictions == vlabels).sum().item()\n",
    "\n",
    "    # Averaging the loss for all samples in the validation set\n",
    "    running_vloss /= (i + 1)\n",
    "\n",
    "    # Calculate accuracy based on the total samples in the fold (val_sampler_size)\n",
    "    validation_accuracy = running_vcorrect / val_sampler_size\n",
    "\n",
    "    return running_vloss, validation_accuracy\n",
    "\n",
    "\n",
    "def train_multiple_folds(n_epochs, batch_size, model, splits, writer, optimizer, loss_fn):\n",
    "\n",
    "    best_fold_vloss = 1_000\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "        print(f\"FOLD {fold+1}\")\n",
    "\n",
    "        avg_tloss = 0.\n",
    "        avg_tacc = 0.\n",
    "        avg_vloss = 0.\n",
    "        avg_vacc = 0.\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        train_sampler_size = len(train_sampler)\n",
    "        val_sampler_size = len(val_sampler)\n",
    "\n",
    "        for epoch in range(n_epochs): \n",
    "            train_loss, train_correct = train_epoch(piece_to_move_net, optimizer, train_loader, loss_fn, train_sampler_size)\n",
    "            val_loss, val_correct = validation_epoch(piece_to_move_net, val_loader, loss_fn, val_sampler_size)\n",
    "\n",
    "            avg_tloss += train_loss\n",
    "            avg_tacc += train_correct\n",
    "            avg_vloss += val_loss\n",
    "            avg_vacc += val_correct\n",
    "\n",
    "            print(f\"Epoch: {epoch} Train Loss: {train_loss}, Valid Loss: {val_loss} | Train Acc: {train_correct}, Valid Acc: {val_correct}\")\n",
    "\n",
    "\n",
    "        avg_tloss /= (epoch + 1)\n",
    "        avg_tacc /= (epoch + 1)\n",
    "        avg_vloss /= (epoch + 1)\n",
    "        avg_vacc /= (epoch + 1)\n",
    "\n",
    "        # Adding insights\n",
    "        writer.add_scalars(\"Loss\", {\"Training\": avg_tloss, \"Validation\": avg_vloss}, fold + 1)\n",
    "        writer.add_scalars(\"Accuracy\", {\"Training\": avg_tacc, \"Validation\": avg_vacc}, fold + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Saving the model if the loss on the validation is lower than the best one\n",
    "        if avg_vloss < best_fold_vloss:\n",
    "            best_fold_vloss = avg_vloss\n",
    "            model_path = f\"models/piece_to_move_net_{timestamp}_{fold}\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "K = 10\n",
    "# Logs training statistics for TensorBoard visualization\n",
    "writer = SummaryWriter(f\"runs/piece_to_move_{timestamp}\")  \n",
    "splits = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "train_multiple_folds(EPOCHS, BATCH_SIZE, piece_to_move_net, splits, writer, optimizer, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(board: chess.Board, outputs: np.arry, labels: np.array) -> np.array:\n",
    "    \"\"\"Creates mask with the position of the pieces that are able to move\"\"\"\n",
    "\n",
    "    mask = np.zeros((8,8)) # 8x8 mask for the chessboard\n",
    "\n",
    "    # Obtaining legal moves from the board\n",
    "    legal_moves = list(board.legal_moves)\n",
    "\n",
    "    # Indicating with 1s the valid squares\n",
    "    for move in legal_moves:\n",
    "        to_square = move.to_square\n",
    "        to_row, to_col = divmod(to_square, 8)\n",
    "        mask[to_row, to_col] = 1 # A valid square\n",
    "\n",
    "    # Reshaping mask to match output and labels\n",
    "    move_mask = mask.flatten() # Converts 8*8 2D array to a 1D array with 64 elements\n",
    "\n",
    "    masked_outputs = outputs * move_mask\n",
    "    masked_labels = labels * move_mask\n",
    "\n",
    "    return masked_outputs, masked_labels\n",
    "\n",
    "def update_board( board: chess.Board, move: chess.Move ) -> chess.Board:\n",
    "    \"\"\"This function is responsible for updating the board everytime a move is made\"\"\"\n",
    "\n",
    "    board.push(move) # Add move to the board\n",
    "    \n",
    "    return move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CROSS VALIDATION**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    \"\"\"Resets the weights of the model, so the model is trained with randomly initalized weights\"\"\"\n",
    "\n",
    "    # List of layers containing reset parameters\n",
    "    layer_types = [nn.Conv2d, nn.Linear, nn.BatchNorm2d]\n",
    "\n",
    "    # Iterating through all layers of the model\n",
    "    for layer in model.modules():\n",
    "        # Check layers with reset parameters\n",
    "        if type(layer) in layer_types:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
